{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2021-10-08T08:20:53.434808Z",
     "iopub.status.busy": "2021-10-08T08:20:53.434344Z",
     "iopub.status.idle": "2021-10-08T08:20:59.268599Z",
     "shell.execute_reply": "2021-10-08T08:20:59.267856Z",
     "shell.execute_reply.started": "2021-10-08T08:20:53.434735Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "import transformers as ppb\n",
    "import nltk\n",
    "import re\n",
    "import torch.nn as nn "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-08T08:20:59.272583Z",
     "iopub.status.busy": "2021-10-08T08:20:59.272383Z",
     "iopub.status.idle": "2021-10-08T08:21:16.50624Z",
     "shell.execute_reply": "2021-10-08T08:21:16.505475Z",
     "shell.execute_reply.started": "2021-10-08T08:20:59.27256Z"
    }
   },
   "outputs": [],
   "source": [
    "!pip install openpyxl\n",
    "data = pd.read_excel(\"../input/data-222/incident son 6 aylk.xlsx\")\n",
    "data = data[['Description','Caller','Business service','Incident state',\n",
    "              'Impact','Urgency','Category','Subcategory','Assignment group']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-08T08:21:16.507965Z",
     "iopub.status.busy": "2021-10-08T08:21:16.507716Z",
     "iopub.status.idle": "2021-10-08T08:21:17.702531Z",
     "shell.execute_reply": "2021-10-08T08:21:17.701424Z",
     "shell.execute_reply.started": "2021-10-08T08:21:16.507932Z"
    }
   },
   "outputs": [],
   "source": [
    "#txt preprocess\n",
    "def utils_preprocess_text(text,flg_lemma = True , lst_stopwords=None):\n",
    "    text = re.sub(r'[^\\w\\s]', '', str(text).lower().strip())\n",
    "    lst_text = text.split()    \n",
    "    if lst_stopwords is not None: \n",
    "        lst_text = [word for word in lst_text if word not in ## stop_wordleri silme\n",
    "                    lst_stopwords]\n",
    "    \n",
    "\n",
    "    text = \" \".join(lst_text)\n",
    "    return text\n",
    "def textCleaner(df = None , src = 'content' ,dst = 'text_clean',stop_words = 'turkish'):\n",
    "    #bütün dataframi temizlemek icin\n",
    "    df[dst] = df[src].apply(lambda x: \n",
    "          utils_preprocess_text(x, \n",
    "          lst_stopwords=stop_words))\n",
    "with open(\"../input/stop-words/turkish_stop_words.txt\", 'r', encoding = 'utf-8') as fdict:\n",
    "    tr_stp_wrds = []\n",
    "    for line in fdict:\n",
    "        \n",
    "        word = line.strip().split()[0]\n",
    "        tr_stp_wrds.append(word)\n",
    "\n",
    "textCleaner(df = data,src = 'Description',stop_words = tr_stp_wrds)\n",
    "#data_.drop(\"Description\", inplace = True, axis = 1) \n",
    "#data_.reset_index(drop = True, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-04T20:12:46.20515Z",
     "iopub.status.busy": "2021-10-04T20:12:46.204856Z",
     "iopub.status.idle": "2021-10-04T20:12:46.213903Z",
     "shell.execute_reply": "2021-10-04T20:12:46.213135Z",
     "shell.execute_reply.started": "2021-10-04T20:12:46.20512Z"
    }
   },
   "outputs": [],
   "source": [
    "data['Assignment group'].unique().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-08T08:21:17.704695Z",
     "iopub.status.busy": "2021-10-08T08:21:17.70445Z",
     "iopub.status.idle": "2021-10-08T08:23:36.623345Z",
     "shell.execute_reply": "2021-10-08T08:23:36.622594Z",
     "shell.execute_reply.started": "2021-10-08T08:21:17.704664Z"
    }
   },
   "outputs": [],
   "source": [
    "def pre_process(dataset):\n",
    "    \n",
    "    for col in dataset.columns:\n",
    "        unique_things = dataset[col].unique()\n",
    "        #controlling if the datatype is str if it is we can convert it to int\n",
    "        #print(type(col))\n",
    "        if col != 'text_clean':\n",
    "            if isinstance(unique_things[0],str):     \n",
    "                limit = len(unique_things)\n",
    "                for i in range(limit):\n",
    "                    dataset[col] = dataset[col].replace({unique_things[i]:i})\n",
    "                dataset[col] = dataset[col].astype(int)  \n",
    "pre_process(data)\n",
    "data = data.dropna()\n",
    "x_txt,x_rest,y = np.array(data['text_clean']),np.array(data[['Caller' ,'Business service', 'Incident state', 'Impact' ,'Urgency' ,'Category' ,'Subcategory']]),np.array(data['Assignment group'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-08T08:23:36.624907Z",
     "iopub.status.busy": "2021-10-08T08:23:36.624623Z",
     "iopub.status.idle": "2021-10-08T08:23:58.512143Z",
     "shell.execute_reply": "2021-10-08T08:23:58.511303Z",
     "shell.execute_reply.started": "2021-10-08T08:23:36.624874Z"
    }
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "pretrained = 'dbmdz/distilbert-base-turkish-cased'\n",
    "tokenizer = ppb.DistilBertTokenizer.from_pretrained(pretrained)\n",
    "vectorizer =ppb.DistilBertModel.from_pretrained(pretrained).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-08T08:23:58.513606Z",
     "iopub.status.busy": "2021-10-08T08:23:58.513354Z",
     "iopub.status.idle": "2021-10-08T08:26:41.36961Z",
     "shell.execute_reply": "2021-10-08T08:26:41.366841Z",
     "shell.execute_reply.started": "2021-10-08T08:23:58.513572Z"
    }
   },
   "outputs": [],
   "source": [
    "#sentence vectorizing process\n",
    "vectors = []\n",
    "rest_vect = []\n",
    "classes = []\n",
    "max_len = 512\n",
    "for txt_,rest,target in tqdm(zip(x_txt,x_rest,y)):\n",
    "    \n",
    "    try:\n",
    "        \n",
    "        txt_encd = tokenizer.encode(txt_)\n",
    "        padded_txt = torch.tensor(txt_encd + [0] * (max_len - len(txt_encd)))\n",
    "    \n",
    "        input_ids = padded_txt.clone().detach().type(torch.LongTensor).to(device)\n",
    "        if not input_ids.shape[0] > 512:\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            with torch.no_grad():  \n",
    "                t_1 = torch.clamp(vectorizer(input_ids.unsqueeze(0) )[0][:, 0, :],0,1)\n",
    "            vectors.append(t_1)\n",
    "            rest_vect.append(torch.tensor(rest))\n",
    "            classes.append(torch.tensor(target))\n",
    "\n",
    "            \n",
    "        \n",
    "        \n",
    "    except:\n",
    "        print(txt_)\n",
    "        raise Exception\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-06T07:06:59.57793Z",
     "iopub.status.busy": "2021-10-06T07:06:59.577648Z",
     "iopub.status.idle": "2021-10-06T07:06:59.585298Z",
     "shell.execute_reply": "2021-10-06T07:06:59.584611Z",
     "shell.execute_reply.started": "2021-10-06T07:06:59.577895Z"
    }
   },
   "outputs": [],
   "source": [
    "#creating the datasets\n",
    "class dataset():\n",
    "    def __init__(self,sentence_vectors,other_vectors,classes):\n",
    "        self.sentence_vectors = sentence_vectors\n",
    "        self.other_vectors = other_vectors\n",
    "        self.classes = classes\n",
    "    def __getitem__(self, index: int):\n",
    "        #return [list(padded_txt1),list(torch.tensor(self.others[index]).to(device)),]\n",
    "        return [self.sentence_vectors[index],self.other_vectors[index],self.classes[index]]\n",
    "        #return [self.sentences[index][0],self.sentences[index][1]],torch.tensor(self.classes[index])\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.sentence_vectors)\n",
    "    \n",
    "#dataset_ = dataset(x_txt,x_rest,y)\n",
    "dataset_ = dataset(vectors,rest_vect,classes)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-06T07:06:59.587001Z",
     "iopub.status.busy": "2021-10-06T07:06:59.586555Z",
     "iopub.status.idle": "2021-10-06T07:06:59.629746Z",
     "shell.execute_reply": "2021-10-06T07:06:59.628514Z",
     "shell.execute_reply.started": "2021-10-06T07:06:59.586965Z"
    }
   },
   "outputs": [],
   "source": [
    "concated = []\n",
    "for x in tqdm(dataset_):\n",
    "    concated.append([x[0],x[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-06T07:30:25.245884Z",
     "iopub.status.busy": "2021-10-06T07:30:25.245544Z",
     "iopub.status.idle": "2021-10-06T07:30:25.273932Z",
     "shell.execute_reply": "2021-10-06T07:30:25.273215Z",
     "shell.execute_reply.started": "2021-10-06T07:30:25.245852Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split( concated ,dataset_[:][2],test_size=0.1, random_state=42)\n",
    "vectors_train = [X_train[i][0] for i in range(len(X_train))]\n",
    "other_vectors_train = [X_train[i][1] for i in range(len(X_train))]\n",
    "\n",
    "\n",
    "vectors_test = [X_train[i][0] for i in range(len(X_test))]\n",
    "other_vectors_test = [X_train[i][1] for i in range(len(X_test))]\n",
    "\n",
    "data_train = dataset(vectors_train,other_vectors_train,y_train)\n",
    "data_val = dataset(vectors_test,other_vectors_test,y_test) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-06T07:43:18.757999Z",
     "iopub.status.busy": "2021-10-06T07:43:18.757477Z",
     "iopub.status.idle": "2021-10-06T07:43:18.762424Z",
     "shell.execute_reply": "2021-10-06T07:43:18.761694Z",
     "shell.execute_reply.started": "2021-10-06T07:43:18.757965Z"
    }
   },
   "outputs": [],
   "source": [
    "#creating the dataloaders\n",
    "dataloader_train = torch.utils.data.DataLoader(data_train,\n",
    "                                         batch_size = 1,\n",
    "                                         shuffle = True,\n",
    "                                         num_workers = 0)\n",
    "\n",
    "dataloader_val = torch.utils.data.DataLoader(data_val,\n",
    "                                         batch_size = 1,\n",
    "                                         shuffle = True,\n",
    "                                         num_workers = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-06T07:58:51.897656Z",
     "iopub.status.busy": "2021-10-06T07:58:51.89704Z",
     "iopub.status.idle": "2021-10-06T07:58:51.910215Z",
     "shell.execute_reply": "2021-10-06T07:58:51.909319Z",
     "shell.execute_reply.started": "2021-10-06T07:58:51.897621Z"
    }
   },
   "outputs": [],
   "source": [
    "#basic classifier models\n",
    "class classifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(classifier,self).__init__()\n",
    "        \n",
    "        self.linear_0 = nn.Linear(768+7,500)\n",
    "        self.linear_1 = nn.Linear(500,71)\n",
    "        self.softmax = nn.Softmax()\n",
    "        self.leakyrelu = nn.LeakyReLU(0.15)\n",
    "        \n",
    "        \n",
    "        \n",
    "    def forward(self,x): \n",
    "        #x = self.conv1d(x.unsqueeze(0).unsqueeze(0))\n",
    "        #x = x.squeeze()\n",
    "\n",
    "        x = self.linear_0(x)\n",
    "        x = self.leakyrelu(x)\n",
    "        x = self.linear_1(x)\n",
    "        \n",
    "        #x = self.softmax(x)\n",
    "\n",
    "        return x  \n",
    "classifier_model = classifier().to(device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-06T07:59:54.479688Z",
     "iopub.status.busy": "2021-10-06T07:59:54.479431Z",
     "iopub.status.idle": "2021-10-06T07:59:54.527897Z",
     "shell.execute_reply": "2021-10-06T07:59:54.526936Z",
     "shell.execute_reply.started": "2021-10-06T07:59:54.47966Z"
    }
   },
   "outputs": [],
   "source": [
    "#training and validation phase\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(classifier_model.parameters(),lr= 0.01,weight_decay = 0.001)\n",
    "n_epochs = 100\n",
    "valid_loss_min = np.Inf\n",
    "max_len = 100\n",
    "batch_size = 4\n",
    "for epoch in range(1, n_epochs+1):\n",
    "    print(\"Training Process...\")\n",
    "    train_loss = 0.0\n",
    "    valid_loss = 0.0\n",
    "    true_train = 0\n",
    "    true_valid = 0\n",
    "    times = 0\n",
    "    train_total_loss = 0.0\n",
    "    loss = 0\n",
    "    #setting the classifier mode to train\n",
    "    classifier_model.train()\n",
    "    for vector,other_vector,target in tqdm(dataloader_train):\n",
    "        \n",
    "        \n",
    "        try:\n",
    "            \n",
    "            vector = vector.to(device)\n",
    "            other_vector = other_vector.to(device)\n",
    "            #target = target.type(torch.LongTensor).to(device)\n",
    "            target = target.to(dtype = torch.float32).to(device)\n",
    "            #print(vector.squeeze(0).squeeze(0).shape,other_vector.squeeze(0).shape)\n",
    "            \n",
    "            model_input = torch.cat((vector.squeeze(0).squeeze(0),other_vector.squeeze(0)),0).to(device)\n",
    "            output = classifier_model(model_input).to(device)\n",
    "            #print(output)\n",
    "            \n",
    "            pred = torch.argmax(output).unsqueeze(0)\n",
    "            if (pred == target):\n",
    "                true_train += 1            \n",
    "\n",
    "\n",
    "            print(output.shape,target.shape)\n",
    "            loss += criterion(output,target)\n",
    "            print(loss)\n",
    "            asd\n",
    "            train_loss += loss.item()\n",
    "            times+=1\n",
    "            if(times % batch_size == 0):\n",
    "                #gradient accumulation phase\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                loss = 0\n",
    "                train_total_loss+=train_loss\n",
    "                train_loss = 0\n",
    "            \n",
    "            \n",
    "            \n",
    "        \n",
    "        except KeyboardInterrupt:\n",
    "            raise KeyboardInterrupt(\"Interrupt\")\n",
    "        except:\n",
    "            raise Exception\n",
    "            print(\"An Error Occurred in Train\")\n",
    "    #setting the classifier mode to evaluation        \n",
    "    print(\"Validation Process...\")\n",
    "    classifier_model.eval()\n",
    "    for vector,other_vector,target in tqdm(dataloader_val):\n",
    "        try:\n",
    "            vector = vector.to(device)\n",
    "            other_vector = other_vector.to(device)\n",
    "            target = target.type(torch.LongTensor).to(device)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                model_input = torch.cat((vector.squeeze(0).squeeze(0),other_vector.squeeze(0)),0).to(device)\n",
    "                output = classifier_model(model_input)\n",
    "                \n",
    "            pred = torch.argmax(output).unsqueeze(0)\n",
    "            if (pred == target):\n",
    "                true_valid += 1      \n",
    "            loss = criterion(output.unsqueeze(0),target) \n",
    "            valid_loss += loss.item()\n",
    "\n",
    "        except KeyboardInterrupt:\n",
    "            raise KeyboardInterrupt(\"Interrupt\")\n",
    "        except:\n",
    "            raise Exception\n",
    "            print(\"An Error Occurred in Validation\")\n",
    "      \n",
    "    print('Epoch: {} \\tTraining Loss: {:.6f} \\tTraining Accuracy: {:.6f} \\tValidation Loss: {:.6f} \\t Validation  Accuracy: {:.6f}'.format(\n",
    "        epoch, train_total_loss,true_train/len(X_train), valid_loss,true_valid/len(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-06T08:06:53.155698Z",
     "iopub.status.busy": "2021-10-06T08:06:53.155433Z",
     "iopub.status.idle": "2021-10-06T08:06:53.1603Z",
     "shell.execute_reply": "2021-10-06T08:06:53.159612Z",
     "shell.execute_reply.started": "2021-10-06T08:06:53.155672Z"
    }
   },
   "outputs": [],
   "source": [
    "#####This is for LSTM models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-06T08:20:22.467706Z",
     "iopub.status.busy": "2021-10-06T08:20:22.467072Z",
     "iopub.status.idle": "2021-10-06T08:20:22.474424Z",
     "shell.execute_reply": "2021-10-06T08:20:22.473357Z",
     "shell.execute_reply.started": "2021-10-06T08:20:22.467667Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import re\n",
    "import pickle\n",
    "#import tensorflow_addons as tfa\n",
    "#!pip install --upgrade tensorflow\n",
    "#!pip install --upgrade tensorflow-gpu\n",
    "from tensorflow import keras \n",
    "from tensorflow.keras.preprocessing import text, sequence\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.layers import Dense, Embedding, Input\n",
    "from tensorflow.keras.layers import LSTM, Bidirectional, GlobalMaxPool1D, Dropout,GlobalAveragePooling1D,LeakyReLU\n",
    "from tensorflow.keras.models import Sequential,Model\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras import backend as K\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.losses import CategoricalCrossentropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-06T08:16:25.059074Z",
     "iopub.status.busy": "2021-10-06T08:16:25.058423Z",
     "iopub.status.idle": "2021-10-06T08:16:41.082367Z",
     "shell.execute_reply": "2021-10-06T08:16:41.081367Z",
     "shell.execute_reply.started": "2021-10-06T08:16:25.059035Z"
    }
   },
   "outputs": [],
   "source": [
    "!pip install openpyxl\n",
    "data = pd.read_excel(\"../input/data-222/incident son 6 aylk.xlsx\")\n",
    "data = data[['Description','Caller','Business service','Incident state',\n",
    "              'Impact','Urgency','Category','Subcategory','Assignment group']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-06T08:16:41.084695Z",
     "iopub.status.busy": "2021-10-06T08:16:41.084347Z",
     "iopub.status.idle": "2021-10-06T08:16:42.283919Z",
     "shell.execute_reply": "2021-10-06T08:16:42.28312Z",
     "shell.execute_reply.started": "2021-10-06T08:16:41.084641Z"
    }
   },
   "outputs": [],
   "source": [
    "#txt preprocess\n",
    "def utils_preprocess_text(text,flg_lemma = True , lst_stopwords=None):\n",
    "    text = re.sub(r'[^\\w\\s]', '', str(text).lower().strip())\n",
    "    lst_text = text.split()    \n",
    "    if lst_stopwords is not None: \n",
    "        lst_text = [word for word in lst_text if word not in ## stop_wordleri silme\n",
    "                    lst_stopwords]\n",
    "    \n",
    "\n",
    "    text = \" \".join(lst_text)\n",
    "    return text\n",
    "def textCleaner(df = None , src = 'content' ,dst = 'text_clean',stop_words = 'turkish'):\n",
    "    #bütün dataframi temizlemek icin\n",
    "    df[dst] = df[src].apply(lambda x: \n",
    "          utils_preprocess_text(x, \n",
    "          lst_stopwords=stop_words))\n",
    "with open(\"../input/stop-words/turkish_stop_words.txt\", 'r', encoding = 'utf-8') as fdict:\n",
    "    tr_stp_wrds = []\n",
    "    for line in fdict:\n",
    "        \n",
    "        word = line.strip().split()[0]\n",
    "        tr_stp_wrds.append(word)\n",
    "\n",
    "textCleaner(df = data,src = 'Description',stop_words = tr_stp_wrds)\n",
    "#data_.drop(\"Description\", inplace = True, axis = 1) \n",
    "#data_.reset_index(drop = True, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-06T08:16:42.285573Z",
     "iopub.status.busy": "2021-10-06T08:16:42.285294Z",
     "iopub.status.idle": "2021-10-06T08:18:58.817733Z",
     "shell.execute_reply": "2021-10-06T08:18:58.816894Z",
     "shell.execute_reply.started": "2021-10-06T08:16:42.285539Z"
    }
   },
   "outputs": [],
   "source": [
    "def pre_process(dataset):\n",
    "    \n",
    "    for col in dataset.columns:\n",
    "        unique_things = dataset[col].unique()\n",
    "        #controlling if the datatype is str if it is we can convert it to int\n",
    "        #print(type(col))\n",
    "        if col != 'text_clean':\n",
    "            if isinstance(unique_things[0],str):     \n",
    "                limit = len(unique_things)\n",
    "                for i in range(limit):\n",
    "                    dataset[col] = dataset[col].replace({unique_things[i]:i})\n",
    "                dataset[col] = dataset[col].astype(int)  \n",
    "pre_process(data)\n",
    "data = data.dropna()\n",
    "x_txt,x_rest,y = np.array(data['text_clean']),np.array(data[['Caller' ,'Business service', 'Incident state', 'Impact' ,'Urgency' ,'Category' ,'Subcategory']]),np.array(data['Assignment group'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-06T08:18:58.819726Z",
     "iopub.status.busy": "2021-10-06T08:18:58.819484Z",
     "iopub.status.idle": "2021-10-06T08:18:58.824077Z",
     "shell.execute_reply": "2021-10-06T08:18:58.823361Z",
     "shell.execute_reply.started": "2021-10-06T08:18:58.819692Z"
    }
   },
   "outputs": [],
   "source": [
    "max_features = 15000\n",
    "maxlen = 100\n",
    "tokenizer = text.Tokenizer(num_words=max_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-06T08:18:58.825824Z",
     "iopub.status.busy": "2021-10-06T08:18:58.825393Z",
     "iopub.status.idle": "2021-10-06T08:19:00.005657Z",
     "shell.execute_reply": "2021-10-06T08:19:00.004814Z",
     "shell.execute_reply.started": "2021-10-06T08:18:58.825787Z"
    }
   },
   "outputs": [],
   "source": [
    "#X, y = tags_[0]['text_clean'],tags_[0]['Main Category']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split( x_txt, y, test_size=0.33, random_state=42)\n",
    "tokenizer.fit_on_texts(list(X_train))#tokenizerın sadece train datasını görüp tokenleri de train datasına \n",
    "#göre çıkarmasını sağlıyor\n",
    "\n",
    "#kelimeler tokenlere dönüşüyor\n",
    "x_train_ = sequence.pad_sequences(tokenizer.texts_to_sequences(X_train), maxlen=maxlen,padding='post') \n",
    "x_test_ = sequence.pad_sequences(tokenizer.texts_to_sequences(X_test), maxlen=maxlen,padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-06T08:22:55.690605Z",
     "iopub.status.busy": "2021-10-06T08:22:55.689706Z",
     "iopub.status.idle": "2021-10-06T08:22:56.362168Z",
     "shell.execute_reply": "2021-10-06T08:22:56.361453Z",
     "shell.execute_reply.started": "2021-10-06T08:22:55.690561Z"
    }
   },
   "outputs": [],
   "source": [
    "def base_model():\n",
    "    sgd = SGD(learning_rate = 0.05,momentum = 0.6)\n",
    "    embed_size = 32\n",
    "    \n",
    "    inp = Input(shape=(maxlen, ))\n",
    "    x = Embedding(max_features, embed_size)(inp)\n",
    "    x = Bidirectional(LSTM(100, return_sequences=True))(x)\n",
    "    x = GlobalMaxPool1D()(x)\n",
    "    x = Dropout(0.1)(x)\n",
    "    x = Dense(100,activation = LeakyReLU())(x)\n",
    "    x = Dropout(0.1)(x)\n",
    "    x = Dense(1, activation=\"softmax\")(x)\n",
    "    model = Model(inputs=inp, outputs=x)\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                    optimizer=sgd,\n",
    "                    metrics=['accuracy'])\n",
    "    return model\n",
    "model = base_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-06T08:28:13.912382Z",
     "iopub.status.busy": "2021-10-06T08:28:13.911435Z",
     "iopub.status.idle": "2021-10-06T08:30:47.041724Z",
     "shell.execute_reply": "2021-10-06T08:30:47.039894Z",
     "shell.execute_reply.started": "2021-10-06T08:28:13.912309Z"
    }
   },
   "outputs": [],
   "source": [
    "model.fit(x_train_,y_train,validation_data = (x_test_,y_test),batch_size = 4,epochs = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
