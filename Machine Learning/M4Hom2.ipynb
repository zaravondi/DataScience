{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/Applications/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/Applications/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/Applications/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/Applications/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/Applications/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/Applications/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Unnamed: 0  year  age      sex            maritl      race  \\\n",
      "0      231655  2006   18  1. Male  1. Never Married  1. White   \n",
      "1       86582  2004   24  1. Male  1. Never Married  1. White   \n",
      "2      161300  2003   45  1. Male        2. Married  1. White   \n",
      "3      155159  2003   43  1. Male        2. Married  3. Asian   \n",
      "4       11443  2005   50  1. Male       4. Divorced  1. White   \n",
      "\n",
      "         education              region        jobclass          health  \\\n",
      "0     1. < HS Grad  2. Middle Atlantic   1. Industrial       1. <=Good   \n",
      "1  4. College Grad  2. Middle Atlantic  2. Information  2. >=Very Good   \n",
      "2  3. Some College  2. Middle Atlantic   1. Industrial       1. <=Good   \n",
      "3  4. College Grad  2. Middle Atlantic  2. Information  2. >=Very Good   \n",
      "4       2. HS Grad  2. Middle Atlantic  2. Information       1. <=Good   \n",
      "\n",
      "  health_ins        wage  \n",
      "0      2. No   75.043154  \n",
      "1      2. No   70.476020  \n",
      "2     1. Yes  130.982177  \n",
      "3     1. Yes  154.685293  \n",
      "4     1. Yes   75.043154  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split # Import train_test_split function\n",
    "from sklearn import metrics #Import scikit-learn metrics module for accuracy calculation\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn import svm\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from keras import models\n",
    "from keras import layers\n",
    "from keras import optimizers\n",
    "from sklearn import preprocessing\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "\n",
    "\n",
    "# load dataset\n",
    "df = pd.read_csv(\"Wage.csv\")\n",
    "\n",
    "print(df.head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "year          0\n",
       "age           0\n",
       "sex           0\n",
       "maritl        0\n",
       "race          0\n",
       "education     0\n",
       "region        0\n",
       "jobclass      0\n",
       "health        0\n",
       "health_ins    0\n",
       "wage          0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#preprocess\n",
    "len(df['Unnamed: 0'].unique())\n",
    "df.drop( 'Unnamed: 0', inplace = True, axis = 1 )\n",
    "df['sex'] = df['sex'].replace({'1. Male':1})\n",
    "df['maritl'] = df['maritl'].replace({'1. Never Married':1, '2. Married':2, '3. Widowed':3, '4. Divorced':4, '5. Separated':5})\n",
    "df['race'] = df['race'].replace({'1. White':1, '2. Black':2, '3. Asian':3, '4. Other':4})\n",
    "df['education'] = df['education'].replace({'1. < HS Grad':1, '2. HS Grad':2, '3. Some College':3, '4. College Grad':4, '5. Advanced Degree':5})\n",
    "df['region'] = df['region'].replace({'2. Middle Atlantic':2})\n",
    "df['jobclass'] = df['jobclass'].replace({'1. Industrial':1,'2. Information':2})\n",
    "df['health'] = df['health'].replace({'1. <=Good':1,'2. >=Very Good':2})\n",
    "df['health_ins'] = df['health_ins'].replace({'1. Yes':1,'2. No':2})\n",
    "df.isnull().sum()  # null olan değerler var mı kontrolü yapılır.\n",
    "#sex,region  tüm data seti için aynı değeri taşıyor bu sebeple bu featurelar kullanılmamıştır.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[['maritl', 'race', 'education','jobclass','health','health_ins']] # data\n",
    "X = preprocessing.scale(X)\n",
    "y = df.wage # Target variable\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def neural_network(X_train, X_test, y_train, y_test,activationFunct,optimizerFuntion,lossFunction):\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.Dense(128, input_dim=6, activation=activationFunct))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.Dense(1024, activation=activationFunct))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.Dense(1, activation=activationFunct))\n",
    "    model.compile(optimizer=optimizerFuntion,loss=lossFunction, metrics=['mse'])\n",
    "    model.fit(X_train, y_train, epochs=50, batch_size=50, validation_data = (X_test, y_test))\n",
    "    y_pred = model.predict(X_test)\n",
    "    print('MSE of Neural Network:',mean_squared_error(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Applications/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /Applications/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 2100 samples, validate on 900 samples\n",
      "Epoch 1/50\n",
      "2100/2100 [==============================] - 1s 422us/step - loss: 10634.4852 - mse: 10634.4844 - val_loss: 12794.4257 - val_mse: 12794.4258\n",
      "Epoch 2/50\n",
      "2100/2100 [==============================] - 0s 139us/step - loss: 7296.5962 - mse: 7296.5967 - val_loss: 10842.8120 - val_mse: 10842.8135\n",
      "Epoch 3/50\n",
      "2100/2100 [==============================] - 0s 134us/step - loss: 6930.5644 - mse: 6930.5649 - val_loss: 9756.4762 - val_mse: 9756.4766\n",
      "Epoch 4/50\n",
      "2100/2100 [==============================] - 0s 164us/step - loss: 6971.8058 - mse: 6971.8062 - val_loss: 8891.1423 - val_mse: 8891.1416\n",
      "Epoch 5/50\n",
      "2100/2100 [==============================] - 0s 154us/step - loss: 6636.0544 - mse: 6636.0542 - val_loss: 7684.9042 - val_mse: 7684.9038\n",
      "Epoch 6/50\n",
      "2100/2100 [==============================] - 0s 153us/step - loss: 6333.6242 - mse: 6333.6235 - val_loss: 6888.7888 - val_mse: 6888.7891\n",
      "Epoch 7/50\n",
      "2100/2100 [==============================] - 0s 156us/step - loss: 5345.6769 - mse: 5345.6772 - val_loss: 3631.6113 - val_mse: 3631.6113\n",
      "Epoch 8/50\n",
      "2100/2100 [==============================] - 0s 138us/step - loss: 3891.5780 - mse: 3891.5781 - val_loss: 2112.5368 - val_mse: 2112.5366\n",
      "Epoch 9/50\n",
      "2100/2100 [==============================] - 0s 137us/step - loss: 3180.7809 - mse: 3180.7808 - val_loss: 2287.4323 - val_mse: 2287.4326\n",
      "Epoch 10/50\n",
      "2100/2100 [==============================] - 0s 138us/step - loss: 2752.9292 - mse: 2752.9287 - val_loss: 1954.0337 - val_mse: 1954.0334\n",
      "Epoch 11/50\n",
      "2100/2100 [==============================] - 0s 138us/step - loss: 2067.4206 - mse: 2067.4207 - val_loss: 2227.5062 - val_mse: 2227.5061\n",
      "Epoch 12/50\n",
      "2100/2100 [==============================] - 0s 138us/step - loss: 2032.9897 - mse: 2032.9897 - val_loss: 1902.5311 - val_mse: 1902.5312\n",
      "Epoch 13/50\n",
      "2100/2100 [==============================] - 0s 137us/step - loss: 1958.5289 - mse: 1958.5291 - val_loss: 1869.4794 - val_mse: 1869.4797\n",
      "Epoch 14/50\n",
      "2100/2100 [==============================] - 0s 136us/step - loss: 1714.9005 - mse: 1714.9006 - val_loss: 1696.1887 - val_mse: 1696.1887\n",
      "Epoch 15/50\n",
      "2100/2100 [==============================] - 0s 139us/step - loss: 1709.2066 - mse: 1709.2064 - val_loss: 1489.2986 - val_mse: 1489.2986\n",
      "Epoch 16/50\n",
      "2100/2100 [==============================] - 0s 138us/step - loss: 1621.1686 - mse: 1621.1687 - val_loss: 1495.2435 - val_mse: 1495.2435\n",
      "Epoch 17/50\n",
      "2100/2100 [==============================] - 0s 149us/step - loss: 1555.0024 - mse: 1555.0024 - val_loss: 1420.8113 - val_mse: 1420.8114\n",
      "Epoch 18/50\n",
      "2100/2100 [==============================] - 0s 157us/step - loss: 1524.3092 - mse: 1524.3094 - val_loss: 1422.1932 - val_mse: 1422.1932\n",
      "Epoch 19/50\n",
      "2100/2100 [==============================] - 0s 146us/step - loss: 1485.7953 - mse: 1485.7953 - val_loss: 1399.5236 - val_mse: 1399.5236\n",
      "Epoch 20/50\n",
      "2100/2100 [==============================] - 0s 147us/step - loss: 1379.8494 - mse: 1379.8491 - val_loss: 1456.5254 - val_mse: 1456.5253\n",
      "Epoch 21/50\n",
      "2100/2100 [==============================] - 0s 167us/step - loss: 1297.1284 - mse: 1297.1283 - val_loss: 1368.1504 - val_mse: 1368.1504\n",
      "Epoch 22/50\n",
      "2100/2100 [==============================] - 0s 153us/step - loss: 1207.6931 - mse: 1207.6931 - val_loss: 1273.8894 - val_mse: 1273.8894\n",
      "Epoch 23/50\n",
      "2100/2100 [==============================] - 0s 144us/step - loss: 1201.0273 - mse: 1201.0271 - val_loss: 1191.7589 - val_mse: 1191.7589\n",
      "Epoch 24/50\n",
      "2100/2100 [==============================] - 0s 163us/step - loss: 1186.2215 - mse: 1186.2214 - val_loss: 1184.8329 - val_mse: 1184.8328\n",
      "Epoch 25/50\n",
      "2100/2100 [==============================] - 0s 150us/step - loss: 1223.7343 - mse: 1223.7343 - val_loss: 1131.3332 - val_mse: 1131.3333\n",
      "Epoch 26/50\n",
      "2100/2100 [==============================] - 0s 155us/step - loss: 1193.4652 - mse: 1193.4651 - val_loss: 1113.3108 - val_mse: 1113.3108\n",
      "Epoch 27/50\n",
      "2100/2100 [==============================] - 0s 188us/step - loss: 1198.2173 - mse: 1198.2172 - val_loss: 1098.4737 - val_mse: 1098.4736\n",
      "Epoch 28/50\n",
      "2100/2100 [==============================] - 0s 150us/step - loss: 1188.7121 - mse: 1188.7122 - val_loss: 1091.9540 - val_mse: 1091.9539\n",
      "Epoch 29/50\n",
      "2100/2100 [==============================] - 0s 162us/step - loss: 1187.9303 - mse: 1187.9304 - val_loss: 1095.9655 - val_mse: 1095.9655\n",
      "Epoch 30/50\n",
      "2100/2100 [==============================] - 0s 170us/step - loss: 1194.5160 - mse: 1194.5160 - val_loss: 1092.6893 - val_mse: 1092.6892\n",
      "Epoch 31/50\n",
      "2100/2100 [==============================] - 0s 145us/step - loss: 1194.4125 - mse: 1194.4125 - val_loss: 1102.2956 - val_mse: 1102.2957\n",
      "Epoch 32/50\n",
      "2100/2100 [==============================] - 0s 146us/step - loss: 1196.8357 - mse: 1196.8354 - val_loss: 1075.3958 - val_mse: 1075.3959\n",
      "Epoch 33/50\n",
      "2100/2100 [==============================] - 0s 149us/step - loss: 1205.2984 - mse: 1205.2986 - val_loss: 1083.3054 - val_mse: 1083.3053\n",
      "Epoch 34/50\n",
      "2100/2100 [==============================] - 0s 134us/step - loss: 1176.1116 - mse: 1176.1116 - val_loss: 1074.3028 - val_mse: 1074.3027\n",
      "Epoch 35/50\n",
      "2100/2100 [==============================] - 0s 131us/step - loss: 1199.6514 - mse: 1199.6514 - val_loss: 1094.0986 - val_mse: 1094.0986\n",
      "Epoch 36/50\n",
      "2100/2100 [==============================] - 0s 132us/step - loss: 1200.2991 - mse: 1200.2990 - val_loss: 1093.0981 - val_mse: 1093.0980\n",
      "Epoch 37/50\n",
      "2100/2100 [==============================] - 0s 142us/step - loss: 1192.8625 - mse: 1192.8625 - val_loss: 1083.3631 - val_mse: 1083.3630\n",
      "Epoch 38/50\n",
      "2100/2100 [==============================] - 0s 148us/step - loss: 1184.6843 - mse: 1184.6843 - val_loss: 1087.0209 - val_mse: 1087.0210\n",
      "Epoch 39/50\n",
      "2100/2100 [==============================] - 0s 149us/step - loss: 1181.3764 - mse: 1181.3766 - val_loss: 1100.7293 - val_mse: 1100.7291\n",
      "Epoch 40/50\n",
      "2100/2100 [==============================] - 0s 139us/step - loss: 1178.9183 - mse: 1178.9181 - val_loss: 1105.4083 - val_mse: 1105.4082\n",
      "Epoch 41/50\n",
      "2100/2100 [==============================] - 0s 142us/step - loss: 1204.6078 - mse: 1204.6078 - val_loss: 1094.8886 - val_mse: 1094.8885\n",
      "Epoch 42/50\n",
      "2100/2100 [==============================] - 0s 175us/step - loss: 1185.0611 - mse: 1185.0610 - val_loss: 1094.9811 - val_mse: 1094.9812\n",
      "Epoch 43/50\n",
      "2100/2100 [==============================] - 0s 137us/step - loss: 1191.2691 - mse: 1191.2690 - val_loss: 1108.2954 - val_mse: 1108.2955\n",
      "Epoch 44/50\n",
      "2100/2100 [==============================] - 0s 160us/step - loss: 1173.6937 - mse: 1173.6938 - val_loss: 1094.4284 - val_mse: 1094.4283\n",
      "Epoch 45/50\n",
      "2100/2100 [==============================] - 0s 153us/step - loss: 1196.2773 - mse: 1196.2772 - val_loss: 1092.4721 - val_mse: 1092.4720\n",
      "Epoch 46/50\n",
      "2100/2100 [==============================] - 0s 137us/step - loss: 1178.4833 - mse: 1178.4833 - val_loss: 1091.4070 - val_mse: 1091.4070\n",
      "Epoch 47/50\n",
      "2100/2100 [==============================] - 0s 154us/step - loss: 1179.9576 - mse: 1179.9576 - val_loss: 1173.4463 - val_mse: 1173.4463\n",
      "Epoch 48/50\n",
      "2100/2100 [==============================] - 0s 141us/step - loss: 1178.0601 - mse: 1178.0603 - val_loss: 1105.2157 - val_mse: 1105.2157\n",
      "Epoch 49/50\n",
      "2100/2100 [==============================] - 0s 140us/step - loss: 1168.9924 - mse: 1168.9923 - val_loss: 1098.5490 - val_mse: 1098.5488\n",
      "Epoch 50/50\n",
      "2100/2100 [==============================] - 0s 177us/step - loss: 1176.5616 - mse: 1176.5616 - val_loss: 1100.2660 - val_mse: 1100.2661\n",
      "MSE of Neural Network: 1100.2660045643227\n",
      "Train on 2100 samples, validate on 900 samples\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2100/2100 [==============================] - 1s 473us/step - loss: 14044.1298 - mse: 14044.1328 - val_loss: 13883.6443 - val_mse: 13883.6426\n",
      "Epoch 2/50\n",
      "2100/2100 [==============================] - 0s 179us/step - loss: 14044.1296 - mse: 14044.1289 - val_loss: 13883.6443 - val_mse: 13883.6426\n",
      "Epoch 3/50\n",
      "2100/2100 [==============================] - 0s 176us/step - loss: 14044.1299 - mse: 14044.1309 - val_loss: 13883.6443 - val_mse: 13883.6426\n",
      "Epoch 4/50\n",
      "2100/2100 [==============================] - 0s 181us/step - loss: 14044.1298 - mse: 14044.1299 - val_loss: 13883.6443 - val_mse: 13883.6426\n",
      "Epoch 5/50\n",
      "2100/2100 [==============================] - 0s 165us/step - loss: 14044.1296 - mse: 14044.1309 - val_loss: 13883.6443 - val_mse: 13883.6426\n",
      "Epoch 6/50\n",
      "2100/2100 [==============================] - 0s 153us/step - loss: 14044.1298 - mse: 14044.1309 - val_loss: 13883.6443 - val_mse: 13883.6426\n",
      "Epoch 7/50\n",
      "2100/2100 [==============================] - 0s 198us/step - loss: 14044.1297 - mse: 14044.1328 - val_loss: 13883.6443 - val_mse: 13883.6426\n",
      "Epoch 8/50\n",
      "2100/2100 [==============================] - 0s 179us/step - loss: 14044.1298 - mse: 14044.1279 - val_loss: 13883.6443 - val_mse: 13883.6426\n",
      "Epoch 9/50\n",
      "2100/2100 [==============================] - 0s 162us/step - loss: 14044.1298 - mse: 14044.1299 - val_loss: 13883.6443 - val_mse: 13883.6426\n",
      "Epoch 10/50\n",
      "2100/2100 [==============================] - 0s 170us/step - loss: 14044.1297 - mse: 14044.1309 - val_loss: 13883.6443 - val_mse: 13883.6426\n",
      "Epoch 11/50\n",
      "2100/2100 [==============================] - 0s 207us/step - loss: 14044.1297 - mse: 14044.1309 - val_loss: 13883.6443 - val_mse: 13883.6426\n",
      "Epoch 12/50\n",
      "2100/2100 [==============================] - 0s 173us/step - loss: 14044.1296 - mse: 14044.1299 - val_loss: 13883.6443 - val_mse: 13883.6426\n",
      "Epoch 13/50\n",
      "2100/2100 [==============================] - 0s 215us/step - loss: 14044.1298 - mse: 14044.1309 - val_loss: 13883.6443 - val_mse: 13883.6426\n",
      "Epoch 14/50\n",
      "2100/2100 [==============================] - 0s 158us/step - loss: 14044.1297 - mse: 14044.1299 - val_loss: 13883.6443 - val_mse: 13883.6426\n",
      "Epoch 15/50\n",
      "2100/2100 [==============================] - 0s 153us/step - loss: 14044.1296 - mse: 14044.1279 - val_loss: 13883.6443 - val_mse: 13883.6426\n",
      "Epoch 16/50\n",
      "2100/2100 [==============================] - 0s 158us/step - loss: 14044.1298 - mse: 14044.1299 - val_loss: 13883.6443 - val_mse: 13883.6426\n",
      "Epoch 17/50\n",
      "2100/2100 [==============================] - 0s 145us/step - loss: 14044.1297 - mse: 14044.1289 - val_loss: 13883.6443 - val_mse: 13883.6426\n",
      "Epoch 18/50\n",
      "2100/2100 [==============================] - 0s 145us/step - loss: 14044.1298 - mse: 14044.1289 - val_loss: 13883.6443 - val_mse: 13883.6426\n",
      "Epoch 19/50\n",
      "2100/2100 [==============================] - 0s 175us/step - loss: 14044.1298 - mse: 14044.1289 - val_loss: 13883.6443 - val_mse: 13883.6426\n",
      "Epoch 20/50\n",
      "2100/2100 [==============================] - 0s 164us/step - loss: 14044.1296 - mse: 14044.1270 - val_loss: 13883.6443 - val_mse: 13883.6426\n",
      "Epoch 21/50\n",
      "2100/2100 [==============================] - 0s 168us/step - loss: 14044.1297 - mse: 14044.1328 - val_loss: 13883.6443 - val_mse: 13883.6426\n",
      "Epoch 22/50\n",
      "2100/2100 [==============================] - 0s 147us/step - loss: 14044.1299 - mse: 14044.1309 - val_loss: 13883.6443 - val_mse: 13883.6426\n",
      "Epoch 23/50\n",
      "2100/2100 [==============================] - 0s 177us/step - loss: 14044.1296 - mse: 14044.1309 - val_loss: 13883.6443 - val_mse: 13883.6426\n",
      "Epoch 24/50\n",
      "2100/2100 [==============================] - 0s 184us/step - loss: 14044.1297 - mse: 14044.1328 - val_loss: 13883.6443 - val_mse: 13883.6426\n",
      "Epoch 25/50\n",
      "2100/2100 [==============================] - 0s 180us/step - loss: 14044.1298 - mse: 14044.1328 - val_loss: 13883.6443 - val_mse: 13883.6426\n",
      "Epoch 26/50\n",
      "2100/2100 [==============================] - 0s 163us/step - loss: 14044.1294 - mse: 14044.1299 - val_loss: 13883.6443 - val_mse: 13883.6426\n",
      "Epoch 27/50\n",
      "2100/2100 [==============================] - 0s 153us/step - loss: 14044.1296 - mse: 14044.1309 - val_loss: 13883.6443 - val_mse: 13883.6426\n",
      "Epoch 28/50\n",
      "2100/2100 [==============================] - 0s 147us/step - loss: 14044.1299 - mse: 14044.1279 - val_loss: 13883.6443 - val_mse: 13883.6426\n",
      "Epoch 29/50\n",
      "2100/2100 [==============================] - 0s 147us/step - loss: 14044.1297 - mse: 14044.1309 - val_loss: 13883.6443 - val_mse: 13883.6426\n",
      "Epoch 30/50\n",
      "2100/2100 [==============================] - 0s 150us/step - loss: 14044.1298 - mse: 14044.1309 - val_loss: 13883.6443 - val_mse: 13883.6426\n",
      "Epoch 31/50\n",
      "2100/2100 [==============================] - 0s 148us/step - loss: 14044.1298 - mse: 14044.1279 - val_loss: 13883.6443 - val_mse: 13883.6426\n",
      "Epoch 32/50\n",
      "2100/2100 [==============================] - 0s 149us/step - loss: 14044.1298 - mse: 14044.1299 - val_loss: 13883.6443 - val_mse: 13883.6426\n",
      "Epoch 33/50\n",
      "2100/2100 [==============================] - 0s 151us/step - loss: 14044.1296 - mse: 14044.1309 - val_loss: 13883.6443 - val_mse: 13883.6426\n",
      "Epoch 34/50\n",
      "2100/2100 [==============================] - 0s 146us/step - loss: 14044.1297 - mse: 14044.1299 - val_loss: 13883.6443 - val_mse: 13883.6426\n",
      "Epoch 35/50\n",
      "2100/2100 [==============================] - 0s 179us/step - loss: 14044.1299 - mse: 14044.1289 - val_loss: 13883.6443 - val_mse: 13883.6426\n",
      "Epoch 36/50\n",
      "2100/2100 [==============================] - 0s 172us/step - loss: 14044.1297 - mse: 14044.1299 - val_loss: 13883.6443 - val_mse: 13883.6426\n",
      "Epoch 37/50\n",
      "2100/2100 [==============================] - 0s 166us/step - loss: 14044.1296 - mse: 14044.1318 - val_loss: 13883.6443 - val_mse: 13883.6426\n",
      "Epoch 38/50\n",
      "2100/2100 [==============================] - 0s 178us/step - loss: 14044.1296 - mse: 14044.1299 - val_loss: 13883.6443 - val_mse: 13883.6426\n",
      "Epoch 39/50\n",
      "2100/2100 [==============================] - 0s 183us/step - loss: 14044.1297 - mse: 14044.1309 - val_loss: 13883.6443 - val_mse: 13883.6426\n",
      "Epoch 40/50\n",
      "2100/2100 [==============================] - 0s 170us/step - loss: 14044.1298 - mse: 14044.1299 - val_loss: 13883.6443 - val_mse: 13883.6426\n",
      "Epoch 41/50\n",
      "2100/2100 [==============================] - 0s 146us/step - loss: 14044.1297 - mse: 14044.1299 - val_loss: 13883.6443 - val_mse: 13883.6426\n",
      "Epoch 42/50\n",
      "2100/2100 [==============================] - 0s 160us/step - loss: 14044.1299 - mse: 14044.1289 - val_loss: 13883.6443 - val_mse: 13883.6426\n",
      "Epoch 43/50\n",
      "2100/2100 [==============================] - 0s 148us/step - loss: 14044.1296 - mse: 14044.1289 - val_loss: 13883.6443 - val_mse: 13883.6426\n",
      "Epoch 44/50\n",
      "2100/2100 [==============================] - 0s 147us/step - loss: 14044.1295 - mse: 14044.1299 - val_loss: 13883.6443 - val_mse: 13883.6426\n",
      "Epoch 45/50\n",
      "2100/2100 [==============================] - 0s 146us/step - loss: 14044.1298 - mse: 14044.1289 - val_loss: 13883.6443 - val_mse: 13883.6426\n",
      "Epoch 46/50\n",
      "2100/2100 [==============================] - 0s 174us/step - loss: 14044.1298 - mse: 14044.1328 - val_loss: 13883.6443 - val_mse: 13883.6426\n",
      "Epoch 47/50\n",
      "2100/2100 [==============================] - ETA: 0s - loss: 14080.0817 - mse: 14080.082 - 0s 176us/step - loss: 14044.1297 - mse: 14044.1318 - val_loss: 13883.6443 - val_mse: 13883.6426\n",
      "Epoch 48/50\n",
      "2100/2100 [==============================] - 0s 175us/step - loss: 14044.1297 - mse: 14044.1299 - val_loss: 13883.6443 - val_mse: 13883.6426\n",
      "Epoch 49/50\n",
      "2100/2100 [==============================] - 0s 174us/step - loss: 14044.1296 - mse: 14044.1289 - val_loss: 13883.6443 - val_mse: 13883.6426\n",
      "Epoch 50/50\n",
      "2100/2100 [==============================] - 0s 165us/step - loss: 14044.1296 - mse: 14044.1309 - val_loss: 13883.6443 - val_mse: 13883.6426\n",
      "MSE of Neural Network: 13883.644275025523\n",
      "Train on 2100 samples, validate on 900 samples\n",
      "Epoch 1/50\n",
      "2100/2100 [==============================] - 1s 418us/step - loss: 14104.2290 - mse: 14104.2275 - val_loss: 13923.0008 - val_mse: 13923.0010\n",
      "Epoch 2/50\n",
      "2100/2100 [==============================] - 0s 135us/step - loss: 14098.6694 - mse: 14098.6689 - val_loss: 13927.8590 - val_mse: 13927.8613\n",
      "Epoch 3/50\n",
      "2100/2100 [==============================] - 0s 132us/step - loss: 14098.6650 - mse: 14098.6650 - val_loss: 13935.9072 - val_mse: 13935.9082\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/50\n",
      "2100/2100 [==============================] - 0s 131us/step - loss: 14098.6683 - mse: 14098.6699 - val_loss: 13944.2818 - val_mse: 13944.2812\n",
      "Epoch 5/50\n",
      "2100/2100 [==============================] - 0s 131us/step - loss: 14098.6640 - mse: 14098.6631 - val_loss: 13950.2328 - val_mse: 13950.2334\n",
      "Epoch 6/50\n",
      "2100/2100 [==============================] - 0s 130us/step - loss: 14098.6631 - mse: 14098.6641 - val_loss: 13950.5918 - val_mse: 13950.5908\n",
      "Epoch 7/50\n",
      "2100/2100 [==============================] - 0s 131us/step - loss: 14098.6635 - mse: 14098.6641 - val_loss: 13947.7055 - val_mse: 13947.7051\n",
      "Epoch 8/50\n",
      "2100/2100 [==============================] - 0s 130us/step - loss: 14098.6630 - mse: 14098.6641 - val_loss: 13945.2343 - val_mse: 13945.2344\n",
      "Epoch 9/50\n",
      "2100/2100 [==============================] - 0s 130us/step - loss: 14098.6637 - mse: 14098.6650 - val_loss: 13944.0372 - val_mse: 13944.0371\n",
      "Epoch 10/50\n",
      "2100/2100 [==============================] - 0s 132us/step - loss: 14098.6656 - mse: 14098.6680 - val_loss: 13943.6221 - val_mse: 13943.6201\n",
      "Epoch 11/50\n",
      "2100/2100 [==============================] - 0s 134us/step - loss: 14098.6628 - mse: 14098.6621 - val_loss: 13943.5279 - val_mse: 13943.5293\n",
      "Epoch 12/50\n",
      "2100/2100 [==============================] - 0s 131us/step - loss: 14098.6627 - mse: 14098.6631 - val_loss: 13943.5079 - val_mse: 13943.5078\n",
      "Epoch 13/50\n",
      "2100/2100 [==============================] - 0s 133us/step - loss: 14098.6629 - mse: 14098.6631 - val_loss: 13943.5036 - val_mse: 13943.5029\n",
      "Epoch 14/50\n",
      "2100/2100 [==============================] - 0s 130us/step - loss: 14098.6632 - mse: 14098.6631 - val_loss: 13943.5030 - val_mse: 13943.5029\n",
      "Epoch 15/50\n",
      "2100/2100 [==============================] - 0s 130us/step - loss: 14098.6667 - mse: 14098.6680 - val_loss: 13943.5026 - val_mse: 13943.5029\n",
      "Epoch 16/50\n",
      "2100/2100 [==============================] - 0s 130us/step - loss: 14098.6631 - mse: 14098.6631 - val_loss: 13943.5026 - val_mse: 13943.5029\n",
      "Epoch 17/50\n",
      "2100/2100 [==============================] - 0s 130us/step - loss: 14098.6638 - mse: 14098.6650 - val_loss: 13943.5026 - val_mse: 13943.5029\n",
      "Epoch 18/50\n",
      "2100/2100 [==============================] - 0s 132us/step - loss: 14098.6659 - mse: 14098.6670 - val_loss: 13943.5026 - val_mse: 13943.5029\n",
      "Epoch 19/50\n",
      "2100/2100 [==============================] - 0s 130us/step - loss: 14098.6626 - mse: 14098.6641 - val_loss: 13943.5026 - val_mse: 13943.5029\n",
      "Epoch 20/50\n",
      "2100/2100 [==============================] - 0s 131us/step - loss: 14098.6631 - mse: 14098.6641 - val_loss: 13943.5026 - val_mse: 13943.5029\n",
      "Epoch 21/50\n",
      "2100/2100 [==============================] - 0s 130us/step - loss: 14098.6629 - mse: 14098.6631 - val_loss: 13943.5026 - val_mse: 13943.5029\n",
      "Epoch 22/50\n",
      "2100/2100 [==============================] - 0s 134us/step - loss: 14098.6631 - mse: 14098.6650 - val_loss: 13943.5026 - val_mse: 13943.5029\n",
      "Epoch 23/50\n",
      "2100/2100 [==============================] - 0s 159us/step - loss: 14098.6629 - mse: 14098.6621 - val_loss: 13943.5026 - val_mse: 13943.5029\n",
      "Epoch 24/50\n",
      "2100/2100 [==============================] - 0s 141us/step - loss: 14098.6629 - mse: 14098.6641 - val_loss: 13943.5026 - val_mse: 13943.5029\n",
      "Epoch 25/50\n",
      "2100/2100 [==============================] - 0s 167us/step - loss: 14098.6639 - mse: 14098.6660 - val_loss: 13943.5026 - val_mse: 13943.5029\n",
      "Epoch 26/50\n",
      "2100/2100 [==============================] - 0s 144us/step - loss: 14098.6630 - mse: 14098.6621 - val_loss: 13943.5026 - val_mse: 13943.5029\n",
      "Epoch 27/50\n",
      "2100/2100 [==============================] - 0s 135us/step - loss: 14098.6657 - mse: 14098.6660 - val_loss: 13943.5026 - val_mse: 13943.5029\n",
      "Epoch 28/50\n",
      "2100/2100 [==============================] - 0s 152us/step - loss: 14098.6630 - mse: 14098.6621 - val_loss: 13943.5026 - val_mse: 13943.5029\n",
      "Epoch 29/50\n",
      "2100/2100 [==============================] - 0s 153us/step - loss: 14098.6745 - mse: 14098.6738 - val_loss: 13943.5029 - val_mse: 13943.5029\n",
      "Epoch 30/50\n",
      "2100/2100 [==============================] - 0s 134us/step - loss: 14098.6740 - mse: 14098.6729 - val_loss: 13943.5026 - val_mse: 13943.5029\n",
      "Epoch 31/50\n",
      "2100/2100 [==============================] - 0s 135us/step - loss: 14098.6631 - mse: 14098.6650 - val_loss: 13943.5026 - val_mse: 13943.5029\n",
      "Epoch 32/50\n",
      "2100/2100 [==============================] - 0s 131us/step - loss: 14098.6633 - mse: 14098.6631 - val_loss: 13943.5026 - val_mse: 13943.5029\n",
      "Epoch 33/50\n",
      "2100/2100 [==============================] - 0s 132us/step - loss: 14098.6686 - mse: 14098.6689 - val_loss: 13943.5026 - val_mse: 13943.5029\n",
      "Epoch 34/50\n",
      "2100/2100 [==============================] - 0s 131us/step - loss: 14098.6659 - mse: 14098.6670 - val_loss: 13943.5026 - val_mse: 13943.5029\n",
      "Epoch 35/50\n",
      "2100/2100 [==============================] - 0s 131us/step - loss: 14098.6727 - mse: 14098.6719 - val_loss: 13943.5026 - val_mse: 13943.5029\n",
      "Epoch 36/50\n",
      "2100/2100 [==============================] - 0s 153us/step - loss: 14098.6650 - mse: 14098.6660 - val_loss: 13943.5026 - val_mse: 13943.5029\n",
      "Epoch 37/50\n",
      "2100/2100 [==============================] - 0s 132us/step - loss: 14098.7154 - mse: 14098.7139 - val_loss: 13943.4958 - val_mse: 13943.4971\n",
      "Epoch 38/50\n",
      "2100/2100 [==============================] - 0s 130us/step - loss: 14104.0429 - mse: 14104.0420 - val_loss: 13943.5026 - val_mse: 13943.5029\n",
      "Epoch 39/50\n",
      "2100/2100 [==============================] - 0s 154us/step - loss: 14098.6626 - mse: 14098.6621 - val_loss: 13943.5026 - val_mse: 13943.5029\n",
      "Epoch 40/50\n",
      "2100/2100 [==============================] - 0s 131us/step - loss: 14098.7373 - mse: 14098.7373 - val_loss: 13943.5026 - val_mse: 13943.5029\n",
      "Epoch 41/50\n",
      "2100/2100 [==============================] - 0s 148us/step - loss: 14098.7018 - mse: 14098.7021 - val_loss: 13943.5026 - val_mse: 13943.5029\n",
      "Epoch 42/50\n",
      "2100/2100 [==============================] - 0s 133us/step - loss: 14098.6736 - mse: 14098.6729 - val_loss: 13943.5026 - val_mse: 13943.5029\n",
      "Epoch 43/50\n",
      "2100/2100 [==============================] - 0s 132us/step - loss: 14098.6626 - mse: 14098.6631 - val_loss: 13943.5026 - val_mse: 13943.5029\n",
      "Epoch 44/50\n",
      "2100/2100 [==============================] - 0s 131us/step - loss: 14098.6629 - mse: 14098.6641 - val_loss: 13943.5026 - val_mse: 13943.5029\n",
      "Epoch 45/50\n",
      "2100/2100 [==============================] - 0s 133us/step - loss: 14098.6626 - mse: 14098.6631 - val_loss: 13943.5026 - val_mse: 13943.5029\n",
      "Epoch 46/50\n",
      "2100/2100 [==============================] - 0s 133us/step - loss: 14098.6626 - mse: 14098.6641 - val_loss: 13943.5026 - val_mse: 13943.5029\n",
      "Epoch 47/50\n",
      "2100/2100 [==============================] - 0s 131us/step - loss: 14098.6629 - mse: 14098.6641 - val_loss: 13943.5026 - val_mse: 13943.5029\n",
      "Epoch 48/50\n",
      "2100/2100 [==============================] - 0s 131us/step - loss: 14098.6627 - mse: 14098.6631 - val_loss: 13943.5026 - val_mse: 13943.5029\n",
      "Epoch 49/50\n",
      "2100/2100 [==============================] - 0s 132us/step - loss: 14098.6673 - mse: 14098.6689 - val_loss: 13943.5026 - val_mse: 13943.5029\n",
      "Epoch 50/50\n",
      "2100/2100 [==============================] - 0s 134us/step - loss: 14098.6630 - mse: 14098.6611 - val_loss: 13943.5026 - val_mse: 13943.5029\n",
      "MSE of Neural Network: 13943.502662025901\n"
     ]
    }
   ],
   "source": [
    "#different activation functions:\n",
    "neural_network(X_train, X_test, y_train, y_test,'relu','adam','mse') #MSE of Neural Network: 1100.2660045643227\n",
    "neural_network(X_train, X_test, y_train, y_test,'softmax','adam','mse')#MSE of Neural Network: 13883.644275025523\n",
    "neural_network(X_train, X_test, y_train, y_test,'sigmoid','adam','mse')#MSE of Neural Network: 13943.184455573968"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2100 samples, validate on 900 samples\n",
      "Epoch 1/50\n",
      "2100/2100 [==============================] - 1s 648us/step - loss: 10212.6925 - mse: 10212.6914 - val_loss: 9807.1537 - val_mse: 9807.1543\n",
      "Epoch 2/50\n",
      "2100/2100 [==============================] - 0s 166us/step - loss: 6690.3877 - mse: 6690.3877 - val_loss: 6329.4118 - val_mse: 6329.4111\n",
      "Epoch 3/50\n",
      "2100/2100 [==============================] - 0s 163us/step - loss: 5428.1305 - mse: 5428.1309 - val_loss: 3325.9652 - val_mse: 3325.9653\n",
      "Epoch 4/50\n",
      "2100/2100 [==============================] - 0s 160us/step - loss: 4846.1853 - mse: 4846.1851 - val_loss: 2206.6459 - val_mse: 2206.6458\n",
      "Epoch 5/50\n",
      "2100/2100 [==============================] - 0s 161us/step - loss: 4168.1542 - mse: 4168.1543 - val_loss: 1873.6593 - val_mse: 1873.6592\n",
      "Epoch 6/50\n",
      "2100/2100 [==============================] - 0s 163us/step - loss: 3901.6695 - mse: 3901.6694 - val_loss: 1791.9930 - val_mse: 1791.9930\n",
      "Epoch 7/50\n",
      "2100/2100 [==============================] - 0s 164us/step - loss: 3493.9077 - mse: 3493.9075 - val_loss: 1940.9427 - val_mse: 1940.9426\n",
      "Epoch 8/50\n",
      "2100/2100 [==============================] - 0s 164us/step - loss: 2974.0485 - mse: 2974.0491 - val_loss: 4487.6846 - val_mse: 4487.6846\n",
      "Epoch 9/50\n",
      "2100/2100 [==============================] - 0s 164us/step - loss: 2335.1276 - mse: 2335.1274 - val_loss: 7314.8703 - val_mse: 7314.8696\n",
      "Epoch 10/50\n",
      "2100/2100 [==============================] - 0s 157us/step - loss: 2121.0841 - mse: 2121.0840 - val_loss: 7058.8517 - val_mse: 7058.8521\n",
      "Epoch 11/50\n",
      "2100/2100 [==============================] - 0s 155us/step - loss: 2080.4552 - mse: 2080.4551 - val_loss: 7520.3613 - val_mse: 7520.3613\n",
      "Epoch 12/50\n",
      "2100/2100 [==============================] - 0s 151us/step - loss: 2048.7534 - mse: 2048.7537 - val_loss: 6233.4188 - val_mse: 6233.4185\n",
      "Epoch 13/50\n",
      "2100/2100 [==============================] - 0s 156us/step - loss: 1939.5775 - mse: 1939.5773 - val_loss: 4705.6388 - val_mse: 4705.6387\n",
      "Epoch 14/50\n",
      "2100/2100 [==============================] - 0s 151us/step - loss: 1811.0308 - mse: 1811.0312 - val_loss: 3415.8443 - val_mse: 3415.8445\n",
      "Epoch 15/50\n",
      "2100/2100 [==============================] - 0s 156us/step - loss: 1783.9037 - mse: 1783.9039 - val_loss: 2975.8399 - val_mse: 2975.8398\n",
      "Epoch 16/50\n",
      "2100/2100 [==============================] - 0s 155us/step - loss: 1821.3772 - mse: 1821.3772 - val_loss: 2610.1954 - val_mse: 2610.1951\n",
      "Epoch 17/50\n",
      "2100/2100 [==============================] - 0s 155us/step - loss: 1761.1756 - mse: 1761.1754 - val_loss: 1954.5367 - val_mse: 1954.5369\n",
      "Epoch 18/50\n",
      "2100/2100 [==============================] - 0s 151us/step - loss: 1698.2404 - mse: 1698.2404 - val_loss: 1788.0174 - val_mse: 1788.0173\n",
      "Epoch 19/50\n",
      "2100/2100 [==============================] - 0s 150us/step - loss: 1765.8193 - mse: 1765.8192 - val_loss: 1674.1281 - val_mse: 1674.1281\n",
      "Epoch 20/50\n",
      "2100/2100 [==============================] - 0s 154us/step - loss: 1588.2536 - mse: 1588.2534 - val_loss: 1535.3701 - val_mse: 1535.3700\n",
      "Epoch 21/50\n",
      "2100/2100 [==============================] - 0s 157us/step - loss: 1531.2025 - mse: 1531.2025 - val_loss: 1471.9237 - val_mse: 1471.9237\n",
      "Epoch 22/50\n",
      "2100/2100 [==============================] - 0s 149us/step - loss: 1422.1250 - mse: 1422.1250 - val_loss: 2066.5363 - val_mse: 2066.5364\n",
      "Epoch 23/50\n",
      "2100/2100 [==============================] - 0s 161us/step - loss: 1243.0188 - mse: 1243.0190 - val_loss: 1934.4357 - val_mse: 1934.4355\n",
      "Epoch 24/50\n",
      "2100/2100 [==============================] - 0s 164us/step - loss: 1234.5104 - mse: 1234.5104 - val_loss: 1521.5019 - val_mse: 1521.5018\n",
      "Epoch 25/50\n",
      "2100/2100 [==============================] - 0s 193us/step - loss: 1220.6923 - mse: 1220.6923 - val_loss: 1260.8251 - val_mse: 1260.8250\n",
      "Epoch 26/50\n",
      "2100/2100 [==============================] - 0s 189us/step - loss: 1241.0274 - mse: 1241.0273 - val_loss: 1226.7822 - val_mse: 1226.7822\n",
      "Epoch 27/50\n",
      "2100/2100 [==============================] - 0s 198us/step - loss: 1223.3500 - mse: 1223.3501 - val_loss: 1132.9645 - val_mse: 1132.9644\n",
      "Epoch 28/50\n",
      "2100/2100 [==============================] - 0s 192us/step - loss: 1218.6784 - mse: 1218.6782 - val_loss: 1133.9456 - val_mse: 1133.9457\n",
      "Epoch 29/50\n",
      "2100/2100 [==============================] - 0s 172us/step - loss: 1216.8783 - mse: 1216.8783 - val_loss: 1127.4785 - val_mse: 1127.4785\n",
      "Epoch 30/50\n",
      "2100/2100 [==============================] - 0s 161us/step - loss: 1215.8488 - mse: 1215.8488 - val_loss: 1122.9720 - val_mse: 1122.9720\n",
      "Epoch 31/50\n",
      "2100/2100 [==============================] - 0s 154us/step - loss: 1221.4973 - mse: 1221.4973 - val_loss: 1132.3893 - val_mse: 1132.3893\n",
      "Epoch 32/50\n",
      "2100/2100 [==============================] - 0s 179us/step - loss: 1207.2317 - mse: 1207.2317 - val_loss: 1145.4972 - val_mse: 1145.4972\n",
      "Epoch 33/50\n",
      "2100/2100 [==============================] - 0s 163us/step - loss: 1224.5134 - mse: 1224.5134 - val_loss: 1127.8974 - val_mse: 1127.8975\n",
      "Epoch 34/50\n",
      "2100/2100 [==============================] - 0s 163us/step - loss: 1219.7757 - mse: 1219.7755 - val_loss: 1125.5764 - val_mse: 1125.5764\n",
      "Epoch 35/50\n",
      "2100/2100 [==============================] - 1s 253us/step - loss: 1203.1778 - mse: 1203.1777 - val_loss: 1114.5165 - val_mse: 1114.5165\n",
      "Epoch 36/50\n",
      "2100/2100 [==============================] - 0s 201us/step - loss: 1217.5754 - mse: 1217.5753 - val_loss: 1117.6095 - val_mse: 1117.6095\n",
      "Epoch 37/50\n",
      "2100/2100 [==============================] - 0s 209us/step - loss: 1207.4627 - mse: 1207.4626 - val_loss: 1130.1007 - val_mse: 1130.1007\n",
      "Epoch 38/50\n",
      "2100/2100 [==============================] - 0s 222us/step - loss: 1204.8317 - mse: 1204.8318 - val_loss: 1133.4861 - val_mse: 1133.4862\n",
      "Epoch 39/50\n",
      "2100/2100 [==============================] - 0s 214us/step - loss: 1210.9409 - mse: 1210.9408 - val_loss: 1122.7332 - val_mse: 1122.7332\n",
      "Epoch 40/50\n",
      "2100/2100 [==============================] - 0s 174us/step - loss: 1207.3733 - mse: 1207.3733 - val_loss: 1164.6324 - val_mse: 1164.6324\n",
      "Epoch 41/50\n",
      "2100/2100 [==============================] - 0s 181us/step - loss: 1212.0839 - mse: 1212.0839 - val_loss: 1135.3005 - val_mse: 1135.3005\n",
      "Epoch 42/50\n",
      "2100/2100 [==============================] - 0s 170us/step - loss: 1204.9410 - mse: 1204.9408 - val_loss: 1110.8399 - val_mse: 1110.8400\n",
      "Epoch 43/50\n",
      "2100/2100 [==============================] - 0s 156us/step - loss: 1195.7229 - mse: 1195.7230 - val_loss: 1103.5333 - val_mse: 1103.5333\n",
      "Epoch 44/50\n",
      "2100/2100 [==============================] - 0s 178us/step - loss: 1194.9109 - mse: 1194.9109 - val_loss: 1109.1216 - val_mse: 1109.1217\n",
      "Epoch 45/50\n",
      "2100/2100 [==============================] - 0s 162us/step - loss: 1200.2310 - mse: 1200.2308 - val_loss: 1132.6047 - val_mse: 1132.6046\n",
      "Epoch 46/50\n",
      "2100/2100 [==============================] - 0s 158us/step - loss: 1211.1272 - mse: 1211.1273 - val_loss: 1130.5654 - val_mse: 1130.5654\n",
      "Epoch 47/50\n",
      "2100/2100 [==============================] - 0s 159us/step - loss: 1204.3485 - mse: 1204.3484 - val_loss: 1119.0602 - val_mse: 1119.0602\n",
      "Epoch 48/50\n",
      "2100/2100 [==============================] - 0s 158us/step - loss: 1206.6772 - mse: 1206.6772 - val_loss: 1125.2511 - val_mse: 1125.2511\n",
      "Epoch 49/50\n",
      "2100/2100 [==============================] - 0s 155us/step - loss: 1183.1950 - mse: 1183.1951 - val_loss: 1117.7993 - val_mse: 1117.7993\n",
      "Epoch 50/50\n",
      "2100/2100 [==============================] - 0s 152us/step - loss: 1198.3744 - mse: 1198.3744 - val_loss: 1137.7995 - val_mse: 1137.7996\n",
      "MSE of Neural Network: 1137.799503737872\n",
      "Train on 2100 samples, validate on 900 samples\n",
      "Epoch 1/50\n",
      "2100/2100 [==============================] - 1s 636us/step - loss: 10947.5925 - mse: 10947.5928 - val_loss: 13632.4918 - val_mse: 13632.4912\n",
      "Epoch 2/50\n",
      "2100/2100 [==============================] - 0s 156us/step - loss: 8607.4801 - mse: 8607.4805 - val_loss: 7641.7328 - val_mse: 7641.7324\n",
      "Epoch 3/50\n",
      "2100/2100 [==============================] - 0s 151us/step - loss: 6616.1217 - mse: 6616.1226 - val_loss: 2831.3854 - val_mse: 2831.3853\n",
      "Epoch 4/50\n",
      "2100/2100 [==============================] - 0s 149us/step - loss: 5434.2510 - mse: 5434.2500 - val_loss: 1733.0135 - val_mse: 1733.0137\n",
      "Epoch 5/50\n",
      "2100/2100 [==============================] - 0s 148us/step - loss: 4522.3784 - mse: 4522.3774 - val_loss: 4045.2642 - val_mse: 4045.2644\n",
      "Epoch 6/50\n",
      "2100/2100 [==============================] - 0s 145us/step - loss: 4246.9581 - mse: 4246.9580 - val_loss: 4752.0560 - val_mse: 4752.0562\n",
      "Epoch 7/50\n",
      "2100/2100 [==============================] - 0s 146us/step - loss: 4143.7494 - mse: 4143.7495 - val_loss: 5919.7342 - val_mse: 5919.7339\n",
      "Epoch 8/50\n",
      "2100/2100 [==============================] - 0s 147us/step - loss: 3847.9294 - mse: 3847.9294 - val_loss: 5176.9080 - val_mse: 5176.9087\n",
      "Epoch 9/50\n",
      "2100/2100 [==============================] - 0s 147us/step - loss: 2951.2501 - mse: 2951.2498 - val_loss: 6438.1066 - val_mse: 6438.1060\n",
      "Epoch 10/50\n",
      "2100/2100 [==============================] - 0s 145us/step - loss: 2304.7144 - mse: 2304.7148 - val_loss: 4415.2648 - val_mse: 4415.2651\n",
      "Epoch 11/50\n",
      "2100/2100 [==============================] - 0s 148us/step - loss: 2072.1571 - mse: 2072.1570 - val_loss: 3745.8844 - val_mse: 3745.8843\n",
      "Epoch 12/50\n",
      "2100/2100 [==============================] - 0s 185us/step - loss: 1891.6556 - mse: 1891.6559 - val_loss: 2083.1178 - val_mse: 2083.1177\n",
      "Epoch 13/50\n",
      "2100/2100 [==============================] - 0s 177us/step - loss: 1834.4248 - mse: 1834.4250 - val_loss: 1999.3545 - val_mse: 1999.3545\n",
      "Epoch 14/50\n",
      "2100/2100 [==============================] - 0s 181us/step - loss: 1442.2935 - mse: 1442.2933 - val_loss: 2277.4057 - val_mse: 2277.4055\n",
      "Epoch 15/50\n",
      "2100/2100 [==============================] - 0s 189us/step - loss: 1278.9087 - mse: 1278.9087 - val_loss: 2426.9115 - val_mse: 2426.9116\n",
      "Epoch 16/50\n",
      "2100/2100 [==============================] - 0s 147us/step - loss: 1253.4223 - mse: 1253.4221 - val_loss: 1695.1456 - val_mse: 1695.1458\n",
      "Epoch 17/50\n",
      "2100/2100 [==============================] - 0s 153us/step - loss: 1225.2921 - mse: 1225.2920 - val_loss: 1662.4894 - val_mse: 1662.4895\n",
      "Epoch 18/50\n",
      "2100/2100 [==============================] - 0s 154us/step - loss: 1212.8408 - mse: 1212.8407 - val_loss: 1504.8064 - val_mse: 1504.8065\n",
      "Epoch 19/50\n",
      "2100/2100 [==============================] - 0s 149us/step - loss: 1228.4250 - mse: 1228.4249 - val_loss: 1251.2609 - val_mse: 1251.2609\n",
      "Epoch 20/50\n",
      "2100/2100 [==============================] - 0s 153us/step - loss: 1246.8578 - mse: 1246.8580 - val_loss: 1216.9528 - val_mse: 1216.9528\n",
      "Epoch 21/50\n",
      "2100/2100 [==============================] - 0s 149us/step - loss: 1217.2703 - mse: 1217.2704 - val_loss: 1221.6688 - val_mse: 1221.6687\n",
      "Epoch 22/50\n",
      "2100/2100 [==============================] - 0s 151us/step - loss: 1224.8725 - mse: 1224.8726 - val_loss: 1139.5654 - val_mse: 1139.5654\n",
      "Epoch 23/50\n",
      "2100/2100 [==============================] - 0s 153us/step - loss: 1226.8460 - mse: 1226.8461 - val_loss: 1119.9501 - val_mse: 1119.9501\n",
      "Epoch 24/50\n",
      "2100/2100 [==============================] - 0s 150us/step - loss: 1219.9926 - mse: 1219.9927 - val_loss: 1218.6230 - val_mse: 1218.6229\n",
      "Epoch 25/50\n",
      "2100/2100 [==============================] - 0s 150us/step - loss: 1215.2403 - mse: 1215.2405 - val_loss: 1170.8231 - val_mse: 1170.8232\n",
      "Epoch 26/50\n",
      "2100/2100 [==============================] - 0s 145us/step - loss: 1210.5912 - mse: 1210.5911 - val_loss: 1143.2809 - val_mse: 1143.2810\n",
      "Epoch 27/50\n",
      "2100/2100 [==============================] - 0s 145us/step - loss: 1232.3934 - mse: 1232.3936 - val_loss: 1155.8291 - val_mse: 1155.8292\n",
      "Epoch 28/50\n",
      "2100/2100 [==============================] - 0s 147us/step - loss: 1225.7447 - mse: 1225.7448 - val_loss: 1288.9172 - val_mse: 1288.9172\n",
      "Epoch 29/50\n",
      "2100/2100 [==============================] - 0s 145us/step - loss: 1223.4182 - mse: 1223.4185 - val_loss: 1151.2879 - val_mse: 1151.2877\n",
      "Epoch 30/50\n",
      "2100/2100 [==============================] - 0s 143us/step - loss: 1213.9917 - mse: 1213.9918 - val_loss: 1113.5163 - val_mse: 1113.5162\n",
      "Epoch 31/50\n",
      "2100/2100 [==============================] - 0s 143us/step - loss: 1228.9855 - mse: 1228.9856 - val_loss: 1258.8501 - val_mse: 1258.8501\n",
      "Epoch 32/50\n",
      "2100/2100 [==============================] - 0s 145us/step - loss: 1216.0955 - mse: 1216.0956 - val_loss: 1154.3577 - val_mse: 1154.3577\n",
      "Epoch 33/50\n",
      "2100/2100 [==============================] - 0s 147us/step - loss: 1219.1464 - mse: 1219.1465 - val_loss: 1112.2325 - val_mse: 1112.2325\n",
      "Epoch 34/50\n",
      "2100/2100 [==============================] - 0s 149us/step - loss: 1212.8884 - mse: 1212.8883 - val_loss: 1155.3428 - val_mse: 1155.3429\n",
      "Epoch 35/50\n",
      "2100/2100 [==============================] - 0s 147us/step - loss: 1205.7992 - mse: 1205.7992 - val_loss: 1162.0305 - val_mse: 1162.0304\n",
      "Epoch 36/50\n",
      "2100/2100 [==============================] - 0s 145us/step - loss: 1209.7856 - mse: 1209.7858 - val_loss: 1145.0547 - val_mse: 1145.0547\n",
      "Epoch 37/50\n",
      "2100/2100 [==============================] - 0s 148us/step - loss: 1197.5280 - mse: 1197.5278 - val_loss: 1134.3344 - val_mse: 1134.3345\n",
      "Epoch 38/50\n",
      "2100/2100 [==============================] - 0s 146us/step - loss: 1205.2654 - mse: 1205.2654 - val_loss: 1091.7838 - val_mse: 1091.7839\n",
      "Epoch 39/50\n",
      "2100/2100 [==============================] - 0s 145us/step - loss: 1213.6558 - mse: 1213.6558 - val_loss: 1168.0958 - val_mse: 1168.0958\n",
      "Epoch 40/50\n",
      "2100/2100 [==============================] - 0s 147us/step - loss: 1214.0036 - mse: 1214.0035 - val_loss: 1196.8662 - val_mse: 1196.8662\n",
      "Epoch 41/50\n",
      "2100/2100 [==============================] - 0s 145us/step - loss: 1210.6405 - mse: 1210.6406 - val_loss: 1105.7341 - val_mse: 1105.7340\n",
      "Epoch 42/50\n",
      "2100/2100 [==============================] - 0s 145us/step - loss: 1208.3658 - mse: 1208.3658 - val_loss: 1126.4589 - val_mse: 1126.4590\n",
      "Epoch 43/50\n",
      "2100/2100 [==============================] - 0s 164us/step - loss: 1202.7038 - mse: 1202.7037 - val_loss: 1108.3316 - val_mse: 1108.3317\n",
      "Epoch 44/50\n",
      "2100/2100 [==============================] - 0s 172us/step - loss: 1203.2745 - mse: 1203.2747 - val_loss: 1090.0228 - val_mse: 1090.0228\n",
      "Epoch 45/50\n",
      "2100/2100 [==============================] - 0s 180us/step - loss: 1192.1103 - mse: 1192.1105 - val_loss: 1173.3668 - val_mse: 1173.3667\n",
      "Epoch 46/50\n",
      "2100/2100 [==============================] - 0s 178us/step - loss: 1208.3491 - mse: 1208.3489 - val_loss: 1101.0769 - val_mse: 1101.0768\n",
      "Epoch 47/50\n",
      "2100/2100 [==============================] - 0s 168us/step - loss: 1199.2724 - mse: 1199.2725 - val_loss: 1091.9879 - val_mse: 1091.9878\n",
      "Epoch 48/50\n",
      "2100/2100 [==============================] - 0s 148us/step - loss: 1193.5722 - mse: 1193.5720 - val_loss: 1115.3840 - val_mse: 1115.3839\n",
      "Epoch 49/50\n",
      "2100/2100 [==============================] - 0s 154us/step - loss: 1212.4234 - mse: 1212.4236 - val_loss: 1155.6917 - val_mse: 1155.6918\n",
      "Epoch 50/50\n",
      "2100/2100 [==============================] - 0s 150us/step - loss: 1184.5835 - mse: 1184.5835 - val_loss: 1120.4422 - val_mse: 1120.4421\n",
      "MSE of Neural Network: 1120.4422203781269\n",
      "Train on 2100 samples, validate on 900 samples\n",
      "Epoch 1/50\n",
      "2100/2100 [==============================] - 1s 704us/step - loss: 8622.8086 - mse: 8622.8096 - val_loss: 6543.4657 - val_mse: 6543.4653\n",
      "Epoch 2/50\n",
      "2100/2100 [==============================] - 0s 172us/step - loss: 5175.4620 - mse: 5175.4624 - val_loss: 2225.7678 - val_mse: 2225.7678\n",
      "Epoch 3/50\n",
      "2100/2100 [==============================] - 0s 168us/step - loss: 3488.0273 - mse: 3488.0271 - val_loss: 4981.5891 - val_mse: 4981.5889\n",
      "Epoch 4/50\n",
      "2100/2100 [==============================] - 0s 166us/step - loss: 2476.3915 - mse: 2476.3916 - val_loss: 20339.9433 - val_mse: 20339.9453\n",
      "Epoch 5/50\n",
      "2100/2100 [==============================] - 0s 170us/step - loss: 2145.9581 - mse: 2145.9578 - val_loss: 26613.4864 - val_mse: 26613.4883\n",
      "Epoch 6/50\n",
      "2100/2100 [==============================] - 0s 170us/step - loss: 1957.8655 - mse: 1957.8656 - val_loss: 14586.1953 - val_mse: 14586.1963\n",
      "Epoch 7/50\n",
      "2100/2100 [==============================] - 0s 170us/step - loss: 1911.6723 - mse: 1911.6724 - val_loss: 9644.5402 - val_mse: 9644.5400\n",
      "Epoch 8/50\n",
      "2100/2100 [==============================] - 0s 169us/step - loss: 1709.3816 - mse: 1709.3816 - val_loss: 8271.5831 - val_mse: 8271.5830\n",
      "Epoch 9/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2100/2100 [==============================] - 0s 170us/step - loss: 1647.1008 - mse: 1647.1007 - val_loss: 5758.2148 - val_mse: 5758.2144\n",
      "Epoch 10/50\n",
      "2100/2100 [==============================] - 0s 168us/step - loss: 1472.2286 - mse: 1472.2284 - val_loss: 3229.5444 - val_mse: 3229.5444\n",
      "Epoch 11/50\n",
      "2100/2100 [==============================] - 0s 168us/step - loss: 1303.3028 - mse: 1303.3027 - val_loss: 3964.4466 - val_mse: 3964.4463\n",
      "Epoch 12/50\n",
      "2100/2100 [==============================] - 0s 170us/step - loss: 1269.2247 - mse: 1269.2249 - val_loss: 2349.8558 - val_mse: 2349.8555\n",
      "Epoch 13/50\n",
      "2100/2100 [==============================] - 0s 170us/step - loss: 1251.5748 - mse: 1251.5747 - val_loss: 1578.6470 - val_mse: 1578.6471\n",
      "Epoch 14/50\n",
      "2100/2100 [==============================] - 0s 175us/step - loss: 1235.3080 - mse: 1235.3080 - val_loss: 1272.6585 - val_mse: 1272.6584\n",
      "Epoch 15/50\n",
      "2100/2100 [==============================] - 0s 173us/step - loss: 1236.4335 - mse: 1236.4336 - val_loss: 1165.4729 - val_mse: 1165.4728\n",
      "Epoch 16/50\n",
      "2100/2100 [==============================] - 0s 174us/step - loss: 1222.9613 - mse: 1222.9611 - val_loss: 1136.3016 - val_mse: 1136.3016\n",
      "Epoch 17/50\n",
      "2100/2100 [==============================] - 0s 176us/step - loss: 1236.0968 - mse: 1236.0967 - val_loss: 1088.5988 - val_mse: 1088.5988\n",
      "Epoch 18/50\n",
      "2100/2100 [==============================] - 0s 178us/step - loss: 1236.1315 - mse: 1236.1315 - val_loss: 1131.7449 - val_mse: 1131.7449\n",
      "Epoch 19/50\n",
      "2100/2100 [==============================] - 0s 176us/step - loss: 1236.4366 - mse: 1236.4365 - val_loss: 1069.9227 - val_mse: 1069.9227\n",
      "Epoch 20/50\n",
      "2100/2100 [==============================] - 0s 175us/step - loss: 1231.1163 - mse: 1231.1165 - val_loss: 1091.0606 - val_mse: 1091.0607\n",
      "Epoch 21/50\n",
      "2100/2100 [==============================] - 0s 170us/step - loss: 1237.0143 - mse: 1237.0140 - val_loss: 1111.8822 - val_mse: 1111.8822\n",
      "Epoch 22/50\n",
      "2100/2100 [==============================] - 0s 173us/step - loss: 1229.1602 - mse: 1229.1602 - val_loss: 1097.6573 - val_mse: 1097.6573\n",
      "Epoch 23/50\n",
      "2100/2100 [==============================] - 0s 176us/step - loss: 1235.0731 - mse: 1235.0732 - val_loss: 1082.8744 - val_mse: 1082.8744\n",
      "Epoch 24/50\n",
      "2100/2100 [==============================] - 0s 174us/step - loss: 1216.0419 - mse: 1216.0420 - val_loss: 1073.4331 - val_mse: 1073.4331\n",
      "Epoch 25/50\n",
      "2100/2100 [==============================] - 0s 176us/step - loss: 1231.6501 - mse: 1231.6503 - val_loss: 1131.0041 - val_mse: 1131.0040\n",
      "Epoch 26/50\n",
      "2100/2100 [==============================] - 0s 176us/step - loss: 1219.4992 - mse: 1219.4991 - val_loss: 1138.0372 - val_mse: 1138.0371\n",
      "Epoch 27/50\n",
      "2100/2100 [==============================] - 0s 178us/step - loss: 1225.9330 - mse: 1225.9330 - val_loss: 1099.0036 - val_mse: 1099.0037\n",
      "Epoch 28/50\n",
      "2100/2100 [==============================] - 0s 177us/step - loss: 1214.4885 - mse: 1214.4885 - val_loss: 1086.7349 - val_mse: 1086.7350\n",
      "Epoch 29/50\n",
      "2100/2100 [==============================] - 0s 170us/step - loss: 1214.3312 - mse: 1214.3313 - val_loss: 1086.6354 - val_mse: 1086.6354\n",
      "Epoch 30/50\n",
      "2100/2100 [==============================] - 0s 170us/step - loss: 1215.9724 - mse: 1215.9723 - val_loss: 1088.5832 - val_mse: 1088.5833\n",
      "Epoch 31/50\n",
      "2100/2100 [==============================] - 0s 166us/step - loss: 1216.8346 - mse: 1216.8346 - val_loss: 1111.5517 - val_mse: 1111.5516\n",
      "Epoch 32/50\n",
      "2100/2100 [==============================] - 0s 166us/step - loss: 1220.9996 - mse: 1220.9995 - val_loss: 1078.2768 - val_mse: 1078.2770\n",
      "Epoch 33/50\n",
      "2100/2100 [==============================] - 0s 168us/step - loss: 1229.0320 - mse: 1229.0320 - val_loss: 1076.9390 - val_mse: 1076.9391\n",
      "Epoch 34/50\n",
      "2100/2100 [==============================] - 0s 169us/step - loss: 1222.4504 - mse: 1222.4502 - val_loss: 1117.2762 - val_mse: 1117.2764\n",
      "Epoch 35/50\n",
      "2100/2100 [==============================] - 0s 172us/step - loss: 1235.4916 - mse: 1235.4917 - val_loss: 1092.8440 - val_mse: 1092.8439\n",
      "Epoch 36/50\n",
      "2100/2100 [==============================] - 0s 203us/step - loss: 1218.0414 - mse: 1218.0414 - val_loss: 1082.9746 - val_mse: 1082.9745\n",
      "Epoch 37/50\n",
      "2100/2100 [==============================] - 0s 190us/step - loss: 1215.2889 - mse: 1215.2888 - val_loss: 1117.7837 - val_mse: 1117.7836\n",
      "Epoch 38/50\n",
      "2100/2100 [==============================] - 0s 194us/step - loss: 1225.8661 - mse: 1225.8661 - val_loss: 1116.9502 - val_mse: 1116.9502\n",
      "Epoch 39/50\n",
      "2100/2100 [==============================] - 0s 203us/step - loss: 1233.0468 - mse: 1233.0465 - val_loss: 1095.3756 - val_mse: 1095.3757\n",
      "Epoch 40/50\n",
      "2100/2100 [==============================] - 0s 168us/step - loss: 1217.5582 - mse: 1217.5582 - val_loss: 1078.2929 - val_mse: 1078.2928\n",
      "Epoch 41/50\n",
      "2100/2100 [==============================] - 0s 167us/step - loss: 1205.9178 - mse: 1205.9176 - val_loss: 1091.1488 - val_mse: 1091.1488\n",
      "Epoch 42/50\n",
      "2100/2100 [==============================] - 0s 169us/step - loss: 1203.6098 - mse: 1203.6097 - val_loss: 1095.1159 - val_mse: 1095.1158\n",
      "Epoch 43/50\n",
      "2100/2100 [==============================] - 0s 166us/step - loss: 1213.3573 - mse: 1213.3572 - val_loss: 1085.6885 - val_mse: 1085.6886\n",
      "Epoch 44/50\n",
      "2100/2100 [==============================] - 0s 169us/step - loss: 1233.3475 - mse: 1233.3475 - val_loss: 1069.2380 - val_mse: 1069.2380\n",
      "Epoch 45/50\n",
      "2100/2100 [==============================] - 0s 168us/step - loss: 1212.0897 - mse: 1212.0897 - val_loss: 1078.8335 - val_mse: 1078.8335\n",
      "Epoch 46/50\n",
      "2100/2100 [==============================] - 0s 166us/step - loss: 1213.6281 - mse: 1213.6279 - val_loss: 1079.9369 - val_mse: 1079.9369\n",
      "Epoch 47/50\n",
      "2100/2100 [==============================] - 0s 169us/step - loss: 1221.6773 - mse: 1221.6770 - val_loss: 1098.5427 - val_mse: 1098.5426\n",
      "Epoch 48/50\n",
      "2100/2100 [==============================] - 0s 169us/step - loss: 1236.6175 - mse: 1236.6176 - val_loss: 1098.8532 - val_mse: 1098.8533\n",
      "Epoch 49/50\n",
      "2100/2100 [==============================] - 0s 169us/step - loss: 1208.2220 - mse: 1208.2220 - val_loss: 1114.4712 - val_mse: 1114.4712\n",
      "Epoch 50/50\n",
      "2100/2100 [==============================] - 0s 166us/step - loss: 1214.0950 - mse: 1214.0948 - val_loss: 1085.3749 - val_mse: 1085.3749\n",
      "MSE of Neural Network: 1085.374939181862\n",
      "Train on 2100 samples, validate on 900 samples\n",
      "Epoch 1/50\n",
      "2100/2100 [==============================] - 1s 684us/step - loss: 9326.2306 - mse: 9326.2295 - val_loss: 9384.6532 - val_mse: 9384.6533\n",
      "Epoch 2/50\n",
      "2100/2100 [==============================] - 0s 170us/step - loss: 5914.0471 - mse: 5914.0479 - val_loss: 5512.6757 - val_mse: 5512.6763\n",
      "Epoch 3/50\n",
      "2100/2100 [==============================] - 0s 180us/step - loss: 5156.8889 - mse: 5156.8892 - val_loss: 3726.7277 - val_mse: 3726.7275\n",
      "Epoch 4/50\n",
      "2100/2100 [==============================] - 0s 182us/step - loss: 4979.5589 - mse: 4979.5586 - val_loss: 3458.7841 - val_mse: 3458.7842\n",
      "Epoch 5/50\n",
      "2100/2100 [==============================] - 0s 158us/step - loss: 4295.9344 - mse: 4295.9341 - val_loss: 2076.1515 - val_mse: 2076.1514\n",
      "Epoch 6/50\n",
      "2100/2100 [==============================] - 0s 158us/step - loss: 3628.7503 - mse: 3628.7515 - val_loss: 1884.5277 - val_mse: 1884.5276\n",
      "Epoch 7/50\n",
      "2100/2100 [==============================] - 0s 158us/step - loss: 3200.8365 - mse: 3200.8369 - val_loss: 2128.9949 - val_mse: 2128.9951\n",
      "Epoch 8/50\n",
      "2100/2100 [==============================] - 0s 161us/step - loss: 3078.4172 - mse: 3078.4172 - val_loss: 1959.5239 - val_mse: 1959.5239\n",
      "Epoch 9/50\n",
      "2100/2100 [==============================] - 0s 159us/step - loss: 2963.2250 - mse: 2963.2251 - val_loss: 2062.6691 - val_mse: 2062.6692\n",
      "Epoch 10/50\n",
      "2100/2100 [==============================] - 0s 151us/step - loss: 2906.4416 - mse: 2906.4414 - val_loss: 2103.6113 - val_mse: 2103.6116\n",
      "Epoch 11/50\n",
      "2100/2100 [==============================] - 0s 149us/step - loss: 2676.7184 - mse: 2676.7188 - val_loss: 2298.9084 - val_mse: 2298.9084\n",
      "Epoch 12/50\n",
      "2100/2100 [==============================] - 0s 151us/step - loss: 2574.2532 - mse: 2574.2527 - val_loss: 2576.5668 - val_mse: 2576.5669\n",
      "Epoch 13/50\n",
      "2100/2100 [==============================] - 0s 148us/step - loss: 2456.8694 - mse: 2456.8694 - val_loss: 2635.8512 - val_mse: 2635.8511\n",
      "Epoch 14/50\n",
      "2100/2100 [==============================] - 0s 151us/step - loss: 2117.9237 - mse: 2117.9238 - val_loss: 3253.0635 - val_mse: 3253.0637\n",
      "Epoch 15/50\n",
      "2100/2100 [==============================] - 0s 150us/step - loss: 1883.7445 - mse: 1883.7445 - val_loss: 2600.7055 - val_mse: 2600.7053\n",
      "Epoch 16/50\n",
      "2100/2100 [==============================] - 0s 147us/step - loss: 1976.4585 - mse: 1976.4586 - val_loss: 2052.2490 - val_mse: 2052.2493\n",
      "Epoch 17/50\n",
      "2100/2100 [==============================] - 0s 150us/step - loss: 1906.3484 - mse: 1906.3484 - val_loss: 1877.7318 - val_mse: 1877.7319\n",
      "Epoch 18/50\n",
      "2100/2100 [==============================] - 0s 147us/step - loss: 1797.0972 - mse: 1797.0973 - val_loss: 1946.0440 - val_mse: 1946.0441\n",
      "Epoch 19/50\n",
      "2100/2100 [==============================] - 0s 150us/step - loss: 1788.3402 - mse: 1788.3403 - val_loss: 1750.8295 - val_mse: 1750.8296\n",
      "Epoch 20/50\n",
      "2100/2100 [==============================] - 0s 147us/step - loss: 1714.8359 - mse: 1714.8359 - val_loss: 1616.3201 - val_mse: 1616.3202\n",
      "Epoch 21/50\n",
      "2100/2100 [==============================] - 0s 154us/step - loss: 1708.2931 - mse: 1708.2932 - val_loss: 1639.6899 - val_mse: 1639.6899\n",
      "Epoch 22/50\n",
      "2100/2100 [==============================] - 0s 147us/step - loss: 1659.2986 - mse: 1659.2986 - val_loss: 1569.6243 - val_mse: 1569.6241\n",
      "Epoch 23/50\n",
      "2100/2100 [==============================] - 0s 150us/step - loss: 1699.1146 - mse: 1699.1145 - val_loss: 1576.4752 - val_mse: 1576.4752\n",
      "Epoch 24/50\n",
      "2100/2100 [==============================] - 0s 150us/step - loss: 1622.5935 - mse: 1622.5933 - val_loss: 1607.3733 - val_mse: 1607.3733\n",
      "Epoch 25/50\n",
      "2100/2100 [==============================] - 0s 147us/step - loss: 1659.1425 - mse: 1659.1423 - val_loss: 1581.0864 - val_mse: 1581.0865\n",
      "Epoch 26/50\n",
      "2100/2100 [==============================] - 0s 149us/step - loss: 1642.1711 - mse: 1642.1711 - val_loss: 1477.2802 - val_mse: 1477.2803\n",
      "Epoch 27/50\n",
      "2100/2100 [==============================] - 0s 147us/step - loss: 1574.2898 - mse: 1574.2898 - val_loss: 1470.1020 - val_mse: 1470.1022\n",
      "Epoch 28/50\n",
      "2100/2100 [==============================] - 0s 147us/step - loss: 1606.4315 - mse: 1606.4314 - val_loss: 1523.4936 - val_mse: 1523.4935\n",
      "Epoch 29/50\n",
      "2100/2100 [==============================] - 0s 148us/step - loss: 1452.0712 - mse: 1452.0713 - val_loss: 1446.4368 - val_mse: 1446.4366\n",
      "Epoch 30/50\n",
      "2100/2100 [==============================] - 0s 150us/step - loss: 1468.5461 - mse: 1468.5460 - val_loss: 1490.0124 - val_mse: 1490.0125\n",
      "Epoch 31/50\n",
      "2100/2100 [==============================] - 0s 149us/step - loss: 1426.0766 - mse: 1426.0768 - val_loss: 1444.0929 - val_mse: 1444.0929\n",
      "Epoch 32/50\n",
      "2100/2100 [==============================] - 0s 148us/step - loss: 1415.8907 - mse: 1415.8907 - val_loss: 1477.2763 - val_mse: 1477.2762\n",
      "Epoch 33/50\n",
      "2100/2100 [==============================] - 0s 147us/step - loss: 1326.7717 - mse: 1326.7715 - val_loss: 1397.2813 - val_mse: 1397.2814\n",
      "Epoch 34/50\n",
      "2100/2100 [==============================] - 0s 150us/step - loss: 1272.6983 - mse: 1272.6981 - val_loss: 1273.1165 - val_mse: 1273.1166\n",
      "Epoch 35/50\n",
      "2100/2100 [==============================] - 0s 148us/step - loss: 1246.5469 - mse: 1246.5472 - val_loss: 1359.3002 - val_mse: 1359.3002\n",
      "Epoch 36/50\n",
      "2100/2100 [==============================] - 0s 148us/step - loss: 1212.4624 - mse: 1212.4625 - val_loss: 1249.8816 - val_mse: 1249.8815\n",
      "Epoch 37/50\n",
      "2100/2100 [==============================] - 0s 150us/step - loss: 1224.7447 - mse: 1224.7448 - val_loss: 1160.7169 - val_mse: 1160.7169\n",
      "Epoch 38/50\n",
      "2100/2100 [==============================] - 0s 151us/step - loss: 1214.1596 - mse: 1214.1594 - val_loss: 1141.2264 - val_mse: 1141.2264\n",
      "Epoch 39/50\n",
      "2100/2100 [==============================] - 0s 149us/step - loss: 1200.6838 - mse: 1200.6836 - val_loss: 1149.9172 - val_mse: 1149.9172\n",
      "Epoch 40/50\n",
      "2100/2100 [==============================] - 0s 148us/step - loss: 1209.2912 - mse: 1209.2911 - val_loss: 1120.7888 - val_mse: 1120.7889\n",
      "Epoch 41/50\n",
      "2100/2100 [==============================] - 0s 148us/step - loss: 1217.6304 - mse: 1217.6306 - val_loss: 1133.4249 - val_mse: 1133.4248\n",
      "Epoch 42/50\n",
      "2100/2100 [==============================] - 0s 148us/step - loss: 1193.9825 - mse: 1193.9827 - val_loss: 1130.3622 - val_mse: 1130.3622\n",
      "Epoch 43/50\n",
      "2100/2100 [==============================] - 0s 151us/step - loss: 1200.7556 - mse: 1200.7556 - val_loss: 1122.0023 - val_mse: 1122.0023\n",
      "Epoch 44/50\n",
      "2100/2100 [==============================] - 0s 151us/step - loss: 1199.1158 - mse: 1199.1160 - val_loss: 1123.4190 - val_mse: 1123.4189\n",
      "Epoch 45/50\n",
      "2100/2100 [==============================] - 0s 150us/step - loss: 1205.2803 - mse: 1205.2800 - val_loss: 1150.9327 - val_mse: 1150.9326\n",
      "Epoch 46/50\n",
      "2100/2100 [==============================] - 0s 149us/step - loss: 1216.1835 - mse: 1216.1835 - val_loss: 1149.6092 - val_mse: 1149.6091\n",
      "Epoch 47/50\n",
      "2100/2100 [==============================] - 0s 155us/step - loss: 1206.7223 - mse: 1206.7222 - val_loss: 1124.0873 - val_mse: 1124.0874\n",
      "Epoch 48/50\n",
      "2100/2100 [==============================] - 0s 150us/step - loss: 1182.9102 - mse: 1182.9103 - val_loss: 1131.0385 - val_mse: 1131.0386\n",
      "Epoch 49/50\n",
      "2100/2100 [==============================] - 0s 150us/step - loss: 1189.5790 - mse: 1189.5789 - val_loss: 1127.6276 - val_mse: 1127.6276\n",
      "Epoch 50/50\n",
      "2100/2100 [==============================] - 0s 147us/step - loss: 1194.7940 - mse: 1194.7938 - val_loss: 1124.5153 - val_mse: 1124.5155\n",
      "MSE of Neural Network: 1124.5153590447374\n"
     ]
    }
   ],
   "source": [
    "#different optimezer functions:\n",
    "neural_network(X_train, X_test, y_train, y_test,'relu','adam','mse') #MSE of Neural Network: 1137.799503737872\n",
    "neural_network(X_train, X_test, y_train, y_test,'relu','rmsprop','mse')#MSE of Neural Network: 1120.4422203781269\n",
    "neural_network(X_train, X_test, y_train, y_test,'relu','Nadam','mse')#MSE of Neural Network: 1085.374939181862\n",
    "neural_network(X_train, X_test, y_train, y_test,'relu','Adamax','mse')#MSE of Neural Network: 1124.5153590447374\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2100 samples, validate on 900 samples\n",
      "Epoch 1/50\n",
      "2100/2100 [==============================] - 1s 707us/step - loss: 10247.7595 - mse: 10247.7598 - val_loss: 9783.8781 - val_mse: 9783.8779\n",
      "Epoch 2/50\n",
      "2100/2100 [==============================] - 0s 164us/step - loss: 6878.2092 - mse: 6878.2100 - val_loss: 5398.8685 - val_mse: 5398.8682\n",
      "Epoch 3/50\n",
      "2100/2100 [==============================] - 0s 163us/step - loss: 5486.2457 - mse: 5486.2466 - val_loss: 2669.5390 - val_mse: 2669.5388\n",
      "Epoch 4/50\n",
      "2100/2100 [==============================] - 0s 165us/step - loss: 4969.1560 - mse: 4969.1562 - val_loss: 2260.3715 - val_mse: 2260.3716\n",
      "Epoch 5/50\n",
      "2100/2100 [==============================] - 0s 170us/step - loss: 4297.4257 - mse: 4297.4258 - val_loss: 1840.0488 - val_mse: 1840.0487\n",
      "Epoch 6/50\n",
      "2100/2100 [==============================] - 0s 192us/step - loss: 3790.0557 - mse: 3790.0557 - val_loss: 3181.3203 - val_mse: 3181.3203\n",
      "Epoch 7/50\n",
      "2100/2100 [==============================] - 0s 226us/step - loss: 2776.8737 - mse: 2776.8738 - val_loss: 6906.5113 - val_mse: 6906.5117\n",
      "Epoch 8/50\n",
      "2100/2100 [==============================] - 0s 195us/step - loss: 2506.7948 - mse: 2506.7944 - val_loss: 10492.5598 - val_mse: 10492.5596\n",
      "Epoch 9/50\n",
      "2100/2100 [==============================] - 0s 174us/step - loss: 2094.6586 - mse: 2094.6587 - val_loss: 13534.4245 - val_mse: 13534.4248\n",
      "Epoch 10/50\n",
      "2100/2100 [==============================] - 0s 170us/step - loss: 2072.0832 - mse: 2072.0830 - val_loss: 12125.4717 - val_mse: 12125.4727\n",
      "Epoch 11/50\n",
      "2100/2100 [==============================] - 0s 177us/step - loss: 2207.8925 - mse: 2207.8928 - val_loss: 9147.7304 - val_mse: 9147.7305\n",
      "Epoch 12/50\n",
      "2100/2100 [==============================] - 0s 173us/step - loss: 2076.8285 - mse: 2076.8286 - val_loss: 5444.2855 - val_mse: 5444.2856\n",
      "Epoch 13/50\n",
      "2100/2100 [==============================] - 0s 173us/step - loss: 2039.0994 - mse: 2039.0995 - val_loss: 4637.4516 - val_mse: 4637.4517\n",
      "Epoch 14/50\n",
      "2100/2100 [==============================] - 0s 174us/step - loss: 1968.4196 - mse: 1968.4199 - val_loss: 3585.6482 - val_mse: 3585.6487\n",
      "Epoch 15/50\n",
      "2100/2100 [==============================] - 0s 167us/step - loss: 1904.5986 - mse: 1904.5985 - val_loss: 2061.2141 - val_mse: 2061.2141\n",
      "Epoch 16/50\n",
      "2100/2100 [==============================] - 0s 169us/step - loss: 1790.8107 - mse: 1790.8108 - val_loss: 2612.5035 - val_mse: 2612.5037\n",
      "Epoch 17/50\n",
      "2100/2100 [==============================] - 0s 176us/step - loss: 1646.3908 - mse: 1646.3912 - val_loss: 2292.7231 - val_mse: 2292.7231\n",
      "Epoch 18/50\n",
      "2100/2100 [==============================] - 0s 192us/step - loss: 1657.7702 - mse: 1657.7703 - val_loss: 2192.3362 - val_mse: 2192.3362\n",
      "Epoch 19/50\n",
      "2100/2100 [==============================] - 0s 190us/step - loss: 1561.2659 - mse: 1561.2659 - val_loss: 2065.1275 - val_mse: 2065.1274\n",
      "Epoch 20/50\n",
      "2100/2100 [==============================] - 0s 187us/step - loss: 1355.6537 - mse: 1355.6538 - val_loss: 2418.2813 - val_mse: 2418.2817\n",
      "Epoch 21/50\n",
      "2100/2100 [==============================] - 0s 181us/step - loss: 1258.5817 - mse: 1258.5819 - val_loss: 2232.0074 - val_mse: 2232.0073\n",
      "Epoch 22/50\n",
      "2100/2100 [==============================] - 0s 208us/step - loss: 1238.9699 - mse: 1238.9696 - val_loss: 1588.3640 - val_mse: 1588.3640\n",
      "Epoch 23/50\n",
      "2100/2100 [==============================] - 0s 176us/step - loss: 1232.9619 - mse: 1232.9618 - val_loss: 1332.7496 - val_mse: 1332.7494\n",
      "Epoch 24/50\n",
      "2100/2100 [==============================] - 0s 176us/step - loss: 1232.0686 - mse: 1232.0685 - val_loss: 1157.8874 - val_mse: 1157.8875\n",
      "Epoch 25/50\n",
      "2100/2100 [==============================] - 0s 181us/step - loss: 1226.8485 - mse: 1226.8485 - val_loss: 1171.2150 - val_mse: 1171.2150\n",
      "Epoch 26/50\n",
      "2100/2100 [==============================] - 0s 178us/step - loss: 1221.5131 - mse: 1221.5131 - val_loss: 1146.9166 - val_mse: 1146.9165\n",
      "Epoch 27/50\n",
      "2100/2100 [==============================] - 0s 177us/step - loss: 1231.0872 - mse: 1231.0874 - val_loss: 1122.8167 - val_mse: 1122.8167\n",
      "Epoch 28/50\n",
      "2100/2100 [==============================] - 0s 186us/step - loss: 1233.2575 - mse: 1233.2573 - val_loss: 1096.7585 - val_mse: 1096.7587\n",
      "Epoch 29/50\n",
      "2100/2100 [==============================] - 0s 197us/step - loss: 1224.4058 - mse: 1224.4058 - val_loss: 1102.8857 - val_mse: 1102.8859\n",
      "Epoch 30/50\n",
      "2100/2100 [==============================] - 0s 197us/step - loss: 1223.9348 - mse: 1223.9349 - val_loss: 1104.7871 - val_mse: 1104.7871\n",
      "Epoch 31/50\n",
      "2100/2100 [==============================] - 0s 168us/step - loss: 1228.8983 - mse: 1228.8981 - val_loss: 1091.6260 - val_mse: 1091.6261\n",
      "Epoch 32/50\n",
      "2100/2100 [==============================] - 0s 166us/step - loss: 1221.9911 - mse: 1221.9912 - val_loss: 1084.3292 - val_mse: 1084.3292\n",
      "Epoch 33/50\n",
      "2100/2100 [==============================] - 0s 168us/step - loss: 1222.4791 - mse: 1222.4791 - val_loss: 1089.7953 - val_mse: 1089.7953\n",
      "Epoch 34/50\n",
      "2100/2100 [==============================] - 0s 172us/step - loss: 1213.3057 - mse: 1213.3059 - val_loss: 1090.4288 - val_mse: 1090.4287\n",
      "Epoch 35/50\n",
      "2100/2100 [==============================] - 0s 176us/step - loss: 1217.0017 - mse: 1217.0017 - val_loss: 1096.8567 - val_mse: 1096.8567\n",
      "Epoch 36/50\n",
      "2100/2100 [==============================] - 0s 177us/step - loss: 1222.9490 - mse: 1222.9492 - val_loss: 1091.9004 - val_mse: 1091.9004\n",
      "Epoch 37/50\n",
      "2100/2100 [==============================] - 0s 187us/step - loss: 1222.5858 - mse: 1222.5856 - val_loss: 1083.7222 - val_mse: 1083.7223\n",
      "Epoch 38/50\n",
      "2100/2100 [==============================] - 0s 190us/step - loss: 1220.5290 - mse: 1220.5288 - val_loss: 1102.8815 - val_mse: 1102.8815\n",
      "Epoch 39/50\n",
      "2100/2100 [==============================] - 0s 186us/step - loss: 1213.1908 - mse: 1213.1912 - val_loss: 1084.9933 - val_mse: 1084.9932\n",
      "Epoch 40/50\n",
      "2100/2100 [==============================] - 0s 188us/step - loss: 1234.3066 - mse: 1234.3065 - val_loss: 1089.3796 - val_mse: 1089.3796\n",
      "Epoch 41/50\n",
      "2100/2100 [==============================] - 0s 201us/step - loss: 1205.7013 - mse: 1205.7013 - val_loss: 1083.9214 - val_mse: 1083.9215\n",
      "Epoch 42/50\n",
      "2100/2100 [==============================] - 0s 197us/step - loss: 1210.5366 - mse: 1210.5365 - val_loss: 1079.5396 - val_mse: 1079.5397\n",
      "Epoch 43/50\n",
      "2100/2100 [==============================] - 0s 197us/step - loss: 1214.0594 - mse: 1214.0593 - val_loss: 1082.3678 - val_mse: 1082.3677\n",
      "Epoch 44/50\n",
      "2100/2100 [==============================] - 0s 209us/step - loss: 1203.1610 - mse: 1203.1610 - val_loss: 1087.0455 - val_mse: 1087.0455\n",
      "Epoch 45/50\n",
      "2100/2100 [==============================] - 0s 219us/step - loss: 1212.3205 - mse: 1212.3204 - val_loss: 1081.6307 - val_mse: 1081.6306\n",
      "Epoch 46/50\n",
      "2100/2100 [==============================] - 0s 178us/step - loss: 1213.0160 - mse: 1213.0159 - val_loss: 1076.1138 - val_mse: 1076.1138\n",
      "Epoch 47/50\n",
      "2100/2100 [==============================] - 0s 179us/step - loss: 1207.9618 - mse: 1207.9617 - val_loss: 1084.4404 - val_mse: 1084.4403\n",
      "Epoch 48/50\n",
      "2100/2100 [==============================] - 0s 204us/step - loss: 1221.4179 - mse: 1221.4180 - val_loss: 1111.9100 - val_mse: 1111.9100\n",
      "Epoch 49/50\n",
      "2100/2100 [==============================] - 0s 187us/step - loss: 1198.2079 - mse: 1198.2080 - val_loss: 1094.3629 - val_mse: 1094.3629\n",
      "Epoch 50/50\n",
      "2100/2100 [==============================] - 0s 176us/step - loss: 1225.7026 - mse: 1225.7026 - val_loss: 1077.7886 - val_mse: 1077.7886\n",
      "MSE of Neural Network: 1077.7885712970242\n",
      "Train on 2100 samples, validate on 900 samples\n",
      "Epoch 1/50\n",
      "2100/2100 [==============================] - 2s 778us/step - loss: -60.4620 - mse: 13317.7002 - val_loss: 1076.6657 - val_mse: 14064.8682\n",
      "Epoch 2/50\n",
      "2100/2100 [==============================] - 0s 229us/step - loss: -432.8542 - mse: 13061.4268 - val_loss: -1362.5364 - val_mse: 13573.5713\n",
      "Epoch 3/50\n",
      "2100/2100 [==============================] - 0s 186us/step - loss: -586.5886 - mse: 12963.3857 - val_loss: -1507.5611 - val_mse: 13160.6934\n",
      "Epoch 4/50\n",
      "2100/2100 [==============================] - 0s 191us/step - loss: -945.0159 - mse: 12622.6924 - val_loss: -1367.6215 - val_mse: 12981.7773\n",
      "Epoch 5/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2100/2100 [==============================] - 0s 171us/step - loss: -1086.6724 - mse: 12519.4531 - val_loss: -1335.2478 - val_mse: 12760.0957\n",
      "Epoch 6/50\n",
      "2100/2100 [==============================] - 0s 170us/step - loss: -1109.1639 - mse: 12576.1348 - val_loss: -1373.5505 - val_mse: 12669.3975\n",
      "Epoch 7/50\n",
      "2100/2100 [==============================] - 0s 170us/step - loss: -1188.2166 - mse: 12550.2305 - val_loss: -1324.2900 - val_mse: 12775.8154\n",
      "Epoch 8/50\n",
      "2100/2100 [==============================] - 0s 177us/step - loss: -1242.3736 - mse: 12579.6299 - val_loss: -1540.0307 - val_mse: 12718.8174\n",
      "Epoch 9/50\n",
      "2100/2100 [==============================] - 0s 172us/step - loss: -1373.8696 - mse: 12685.2705 - val_loss: -1428.4294 - val_mse: 12836.3789\n",
      "Epoch 10/50\n",
      "2100/2100 [==============================] - 0s 170us/step - loss: -1360.9614 - mse: 12534.9277 - val_loss: -1426.4116 - val_mse: 12859.4688\n",
      "Epoch 11/50\n",
      "2100/2100 [==============================] - 0s 180us/step - loss: -1432.9378 - mse: 12692.6787 - val_loss: -957.9127 - val_mse: 13308.4287\n",
      "Epoch 12/50\n",
      "2100/2100 [==============================] - 0s 163us/step - loss: -1470.2390 - mse: 12684.4473 - val_loss: -1171.2254 - val_mse: 13172.9941\n",
      "Epoch 13/50\n",
      "2100/2100 [==============================] - 0s 161us/step - loss: -1527.5676 - mse: 12588.5752 - val_loss: -693.2210 - val_mse: 13318.8398\n",
      "Epoch 14/50\n",
      "2100/2100 [==============================] - 0s 167us/step - loss: -1629.4053 - mse: 12557.4092 - val_loss: -1006.0505 - val_mse: 13358.6621\n",
      "Epoch 15/50\n",
      "2100/2100 [==============================] - 0s 164us/step - loss: -1724.4011 - mse: 12516.1318 - val_loss: -157.0284 - val_mse: 13487.7920\n",
      "Epoch 16/50\n",
      "2100/2100 [==============================] - 0s 184us/step - loss: -1738.8443 - mse: 12393.7656 - val_loss: -1227.5559 - val_mse: 13229.1914\n",
      "Epoch 17/50\n",
      "2100/2100 [==============================] - 0s 182us/step - loss: -1735.3269 - mse: 12283.3232 - val_loss: -1462.8805 - val_mse: 13141.5771\n",
      "Epoch 18/50\n",
      "2100/2100 [==============================] - 0s 199us/step - loss: -1762.9458 - mse: 12297.4766 - val_loss: -1610.6745 - val_mse: 13077.4307\n",
      "Epoch 19/50\n",
      "2100/2100 [==============================] - 0s 181us/step - loss: -1764.4704 - mse: 12213.8652 - val_loss: -973.6339 - val_mse: 13319.3047\n",
      "Epoch 20/50\n",
      "2100/2100 [==============================] - 0s 189us/step - loss: -1756.3160 - mse: 12228.9043 - val_loss: -1366.3563 - val_mse: 13215.3770\n",
      "Epoch 21/50\n",
      "2100/2100 [==============================] - 0s 194us/step - loss: -1761.6861 - mse: 12173.0410 - val_loss: -1660.6472 - val_mse: 12813.4912\n",
      "Epoch 22/50\n",
      "2100/2100 [==============================] - 0s 237us/step - loss: -1765.1120 - mse: 12167.4121 - val_loss: -1726.5185 - val_mse: 12547.2559\n",
      "Epoch 23/50\n",
      "2100/2100 [==============================] - 0s 172us/step - loss: -1765.1120 - mse: 12168.5547 - val_loss: -1745.3229 - val_mse: 12371.1104\n",
      "Epoch 24/50\n",
      "2100/2100 [==============================] - 0s 203us/step - loss: -1765.1120 - mse: 12173.8281 - val_loss: -1756.1732 - val_mse: 12257.8418\n",
      "Epoch 25/50\n",
      "2100/2100 [==============================] - 0s 192us/step - loss: -1765.1120 - mse: 12170.8740 - val_loss: -1764.3370 - val_mse: 12181.8574\n",
      "Epoch 26/50\n",
      "2100/2100 [==============================] - 0s 210us/step - loss: -1762.5673 - mse: 12210.3291 - val_loss: -1764.3370 - val_mse: 12282.6133\n",
      "Epoch 27/50\n",
      "2100/2100 [==============================] - 0s 180us/step - loss: -1765.1120 - mse: 12219.3262 - val_loss: -1764.3370 - val_mse: 12199.0771\n",
      "Epoch 28/50\n",
      "2100/2100 [==============================] - 0s 168us/step - loss: -1765.1120 - mse: 12221.4678 - val_loss: -1764.3370 - val_mse: 12144.0293\n",
      "Epoch 29/50\n",
      "2100/2100 [==============================] - 0s 183us/step - loss: -1765.1120 - mse: 12219.4746 - val_loss: -1764.3370 - val_mse: 12112.9834\n",
      "Epoch 30/50\n",
      "2100/2100 [==============================] - 0s 175us/step - loss: -1765.1120 - mse: 12220.1406 - val_loss: -1764.3370 - val_mse: 12088.4297\n",
      "Epoch 31/50\n",
      "2100/2100 [==============================] - 0s 176us/step - loss: -1765.1120 - mse: 12221.4297 - val_loss: -1764.3370 - val_mse: 12074.2363\n",
      "Epoch 32/50\n",
      "2100/2100 [==============================] - 0s 167us/step - loss: -1765.1120 - mse: 12216.2607 - val_loss: -1764.3370 - val_mse: 12065.1230\n",
      "Epoch 33/50\n",
      "2100/2100 [==============================] - 0s 171us/step - loss: -1756.4503 - mse: 12047.7666 - val_loss: -1712.4243 - val_mse: 12497.9922\n",
      "Epoch 34/50\n",
      "2100/2100 [==============================] - 0s 156us/step - loss: -1764.2344 - mse: 11946.4629 - val_loss: -1760.9759 - val_mse: 12373.4404\n",
      "Epoch 35/50\n",
      "2100/2100 [==============================] - 0s 167us/step - loss: -1765.1120 - mse: 11963.5967 - val_loss: -1764.3370 - val_mse: 12193.9287\n",
      "Epoch 36/50\n",
      "2100/2100 [==============================] - 0s 176us/step - loss: -1765.1120 - mse: 11968.3301 - val_loss: -1764.3370 - val_mse: 12073.3564\n",
      "Epoch 37/50\n",
      "2100/2100 [==============================] - 0s 193us/step - loss: -1765.1120 - mse: 11972.7256 - val_loss: -1764.3370 - val_mse: 11987.0723\n",
      "Epoch 38/50\n",
      "2100/2100 [==============================] - 0s 184us/step - loss: -1765.1120 - mse: 11963.3643 - val_loss: -1764.3370 - val_mse: 11935.9307\n",
      "Epoch 39/50\n",
      "2100/2100 [==============================] - 0s 205us/step - loss: -1765.1120 - mse: 11967.3838 - val_loss: -1764.3370 - val_mse: 11897.2021\n",
      "Epoch 40/50\n",
      "2100/2100 [==============================] - 0s 202us/step - loss: -1763.6834 - mse: 11967.8867 - val_loss: -1764.3370 - val_mse: 11872.2070\n",
      "Epoch 41/50\n",
      "2100/2100 [==============================] - 0s 204us/step - loss: -1765.1120 - mse: 11960.7471 - val_loss: -1764.3370 - val_mse: 11854.5840\n",
      "Epoch 42/50\n",
      "2100/2100 [==============================] - 0s 197us/step - loss: -1765.1120 - mse: 11967.9414 - val_loss: -1764.3370 - val_mse: 11842.4023\n",
      "Epoch 43/50\n",
      "2100/2100 [==============================] - 0s 204us/step - loss: -1765.1120 - mse: 11967.1094 - val_loss: -1764.3370 - val_mse: 11837.7588\n",
      "Epoch 44/50\n",
      "2100/2100 [==============================] - 0s 177us/step - loss: -1765.1120 - mse: 11969.2607 - val_loss: -1764.3370 - val_mse: 11834.2490\n",
      "Epoch 45/50\n",
      "2100/2100 [==============================] - 0s 187us/step - loss: -1765.1120 - mse: 11967.2119 - val_loss: -1764.3370 - val_mse: 11829.2119\n",
      "Epoch 46/50\n",
      "2100/2100 [==============================] - 0s 203us/step - loss: -1765.1120 - mse: 11967.0352 - val_loss: -1764.3370 - val_mse: 11823.0957\n",
      "Epoch 47/50\n",
      "2100/2100 [==============================] - 0s 198us/step - loss: -1765.1120 - mse: 11968.5400 - val_loss: -1764.3370 - val_mse: 11824.4834\n",
      "Epoch 48/50\n",
      "2100/2100 [==============================] - 0s 220us/step - loss: -1764.5998 - mse: 11966.8701 - val_loss: -1758.8444 - val_mse: 11798.6436\n",
      "Epoch 49/50\n",
      "2100/2100 [==============================] - 0s 212us/step - loss: -1764.3226 - mse: 11860.7666 - val_loss: -1764.3370 - val_mse: 11746.9727\n",
      "Epoch 50/50\n",
      "2100/2100 [==============================] - 0s 208us/step - loss: -1764.6210 - mse: 11827.5098 - val_loss: -1764.3370 - val_mse: 11886.4570\n",
      "MSE of Neural Network: 11886.455667441078\n"
     ]
    }
   ],
   "source": [
    "#different loss functions:\n",
    "neural_network(X_train, X_test, y_train, y_test,'relu','adam','mse') #MSE of Neural Network: 1077.7885712970242\n",
    "neural_network(X_train, X_test, y_train, y_test,'relu','rmsprop','binary_crossentropy')#MSE of Neural Network: 11886.455667441078\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2100 samples, validate on 900 samples\n",
      "Epoch 1/50\n",
      "2100/2100 [==============================] - 1s 531us/step - loss: -1130.2670 - mse: 14051.6348 - val_loss: -1760.2299 - val_mse: 13883.6426\n",
      "Epoch 2/50\n",
      "2100/2100 [==============================] - 0s 73us/step - loss: -1764.0082 - mse: 14044.1309 - val_loss: -1764.3370 - val_mse: 13883.6426\n",
      "Epoch 3/50\n",
      "2100/2100 [==============================] - 0s 86us/step - loss: -1765.0440 - mse: 14044.1299 - val_loss: -1764.3370 - val_mse: 13883.6426\n",
      "Epoch 4/50\n",
      "2100/2100 [==============================] - 0s 85us/step - loss: -1765.1120 - mse: 14044.1299 - val_loss: -1764.3370 - val_mse: 13883.6426\n",
      "Epoch 5/50\n",
      "2100/2100 [==============================] - 0s 79us/step - loss: -1765.1120 - mse: 14044.1309 - val_loss: -1764.3370 - val_mse: 13883.6426\n",
      "Epoch 6/50\n",
      "2100/2100 [==============================] - 0s 80us/step - loss: -1765.1120 - mse: 14044.1318 - val_loss: -1764.3370 - val_mse: 13883.6426\n",
      "Epoch 7/50\n",
      "2100/2100 [==============================] - 0s 77us/step - loss: -1765.1120 - mse: 14044.1279 - val_loss: -1764.3370 - val_mse: 13883.6426\n",
      "Epoch 8/50\n",
      "2100/2100 [==============================] - 0s 83us/step - loss: -1765.1120 - mse: 14044.1318 - val_loss: -1764.3370 - val_mse: 13883.6426\n",
      "Epoch 9/50\n",
      "2100/2100 [==============================] - 0s 79us/step - loss: -1765.1120 - mse: 14044.1289 - val_loss: -1764.3370 - val_mse: 13883.6426\n",
      "Epoch 10/50\n",
      "2100/2100 [==============================] - 0s 78us/step - loss: -1765.1120 - mse: 14044.1279 - val_loss: -1764.3370 - val_mse: 13883.6426\n",
      "Epoch 11/50\n",
      "2100/2100 [==============================] - 0s 88us/step - loss: -1765.1120 - mse: 14044.1289 - val_loss: -1764.3370 - val_mse: 13883.6426\n",
      "Epoch 12/50\n",
      "2100/2100 [==============================] - 0s 109us/step - loss: -1765.1120 - mse: 14044.1289 - val_loss: -1764.3370 - val_mse: 13883.6426\n",
      "Epoch 13/50\n",
      "2100/2100 [==============================] - 0s 106us/step - loss: -1765.1120 - mse: 14044.1289 - val_loss: -1764.3370 - val_mse: 13883.6426\n",
      "Epoch 14/50\n",
      "2100/2100 [==============================] - 0s 75us/step - loss: -1765.1120 - mse: 14044.1299 - val_loss: -1764.3370 - val_mse: 13883.6426\n",
      "Epoch 15/50\n",
      "2100/2100 [==============================] - 0s 78us/step - loss: -1765.1120 - mse: 14044.1289 - val_loss: -1764.3370 - val_mse: 13883.6426\n",
      "Epoch 16/50\n",
      "2100/2100 [==============================] - 0s 81us/step - loss: -1765.1120 - mse: 14044.1279 - val_loss: -1764.3370 - val_mse: 13883.6426\n",
      "Epoch 17/50\n",
      "2100/2100 [==============================] - 0s 88us/step - loss: -1765.1120 - mse: 14044.1318 - val_loss: -1764.3370 - val_mse: 13883.6426\n",
      "Epoch 18/50\n",
      "2100/2100 [==============================] - 0s 79us/step - loss: -1765.1120 - mse: 14044.1299 - val_loss: -1764.3370 - val_mse: 13883.6426\n",
      "Epoch 19/50\n",
      "2100/2100 [==============================] - 0s 76us/step - loss: -1765.1120 - mse: 14044.1318 - val_loss: -1764.3370 - val_mse: 13883.6426\n",
      "Epoch 20/50\n",
      "2100/2100 [==============================] - 0s 75us/step - loss: -1765.1120 - mse: 14044.1299 - val_loss: -1764.3370 - val_mse: 13883.6426\n",
      "Epoch 21/50\n",
      "2100/2100 [==============================] - 0s 76us/step - loss: -1765.1120 - mse: 14044.1289 - val_loss: -1764.3370 - val_mse: 13883.6426\n",
      "Epoch 22/50\n",
      "2100/2100 [==============================] - 0s 75us/step - loss: -1765.1120 - mse: 14044.1299 - val_loss: -1764.3370 - val_mse: 13883.6426\n",
      "Epoch 23/50\n",
      "2100/2100 [==============================] - 0s 76us/step - loss: -1765.1120 - mse: 14044.1299 - val_loss: -1764.3370 - val_mse: 13883.6426\n",
      "Epoch 24/50\n",
      "2100/2100 [==============================] - 0s 75us/step - loss: -1765.1120 - mse: 14044.1289 - val_loss: -1764.3370 - val_mse: 13883.6426\n",
      "Epoch 25/50\n",
      "2100/2100 [==============================] - 0s 77us/step - loss: -1765.1120 - mse: 14044.1279 - val_loss: -1764.3370 - val_mse: 13883.6426\n",
      "Epoch 26/50\n",
      "2100/2100 [==============================] - 0s 74us/step - loss: -1765.1120 - mse: 14044.1309 - val_loss: -1764.3370 - val_mse: 13883.6426\n",
      "Epoch 27/50\n",
      "2100/2100 [==============================] - 0s 78us/step - loss: -1765.1120 - mse: 14044.1309 - val_loss: -1764.3370 - val_mse: 13883.6426\n",
      "Epoch 28/50\n",
      "2100/2100 [==============================] - 0s 94us/step - loss: -1765.1120 - mse: 14044.1279 - val_loss: -1764.3370 - val_mse: 13883.6426\n",
      "Epoch 29/50\n",
      "2100/2100 [==============================] - 0s 88us/step - loss: -1765.1120 - mse: 14044.1309 - val_loss: -1764.3370 - val_mse: 13883.6426\n",
      "Epoch 30/50\n",
      "2100/2100 [==============================] - 0s 74us/step - loss: -1765.1120 - mse: 14044.1318 - val_loss: -1764.3370 - val_mse: 13883.6426\n",
      "Epoch 31/50\n",
      "2100/2100 [==============================] - 0s 77us/step - loss: -1765.1120 - mse: 14044.1299 - val_loss: -1764.3370 - val_mse: 13883.6426\n",
      "Epoch 32/50\n",
      "2100/2100 [==============================] - 0s 82us/step - loss: -1765.1120 - mse: 14044.1299 - val_loss: -1764.3370 - val_mse: 13883.6426\n",
      "Epoch 33/50\n",
      "2100/2100 [==============================] - 0s 87us/step - loss: -1765.1120 - mse: 14044.1299 - val_loss: -1764.3370 - val_mse: 13883.6426\n",
      "Epoch 34/50\n",
      "2100/2100 [==============================] - 0s 76us/step - loss: -1765.1120 - mse: 14044.1289 - val_loss: -1764.3370 - val_mse: 13883.6426\n",
      "Epoch 35/50\n",
      "2100/2100 [==============================] - 0s 100us/step - loss: -1765.1120 - mse: 14044.1270 - val_loss: -1764.3370 - val_mse: 13883.6426\n",
      "Epoch 36/50\n",
      "2100/2100 [==============================] - 0s 84us/step - loss: -1765.1120 - mse: 14044.1318 - val_loss: -1764.3370 - val_mse: 13883.6426\n",
      "Epoch 37/50\n",
      "2100/2100 [==============================] - 0s 77us/step - loss: -1765.1120 - mse: 14044.1299 - val_loss: -1764.3370 - val_mse: 13883.6426\n",
      "Epoch 38/50\n",
      "2100/2100 [==============================] - 0s 74us/step - loss: -1765.1120 - mse: 14044.1270 - val_loss: -1764.3370 - val_mse: 13883.6426\n",
      "Epoch 39/50\n",
      "2100/2100 [==============================] - 0s 72us/step - loss: -1765.1120 - mse: 14044.1289 - val_loss: -1764.3370 - val_mse: 13883.6426\n",
      "Epoch 40/50\n",
      "2100/2100 [==============================] - 0s 84us/step - loss: -1765.1120 - mse: 14044.1299 - val_loss: -1764.3370 - val_mse: 13883.6426\n",
      "Epoch 41/50\n",
      "2100/2100 [==============================] - 0s 84us/step - loss: -1765.1120 - mse: 14044.1299 - val_loss: -1764.3370 - val_mse: 13883.6426\n",
      "Epoch 42/50\n",
      "2100/2100 [==============================] - 0s 75us/step - loss: -1765.1120 - mse: 14044.1289 - val_loss: -1764.3370 - val_mse: 13883.6426\n",
      "Epoch 43/50\n",
      "2100/2100 [==============================] - 0s 74us/step - loss: -1765.1120 - mse: 14044.1299 - val_loss: -1764.3370 - val_mse: 13883.6426\n",
      "Epoch 44/50\n",
      "2100/2100 [==============================] - 0s 72us/step - loss: -1765.1120 - mse: 14044.1299 - val_loss: -1764.3370 - val_mse: 13883.6426\n",
      "Epoch 45/50\n",
      "2100/2100 [==============================] - 0s 74us/step - loss: -1765.1120 - mse: 14044.1318 - val_loss: -1764.3370 - val_mse: 13883.6426\n",
      "Epoch 46/50\n",
      "2100/2100 [==============================] - 0s 74us/step - loss: -1765.1120 - mse: 14044.1299 - val_loss: -1764.3370 - val_mse: 13883.6426\n",
      "Epoch 47/50\n",
      "2100/2100 [==============================] - 0s 73us/step - loss: -1765.1120 - mse: 14044.1318 - val_loss: -1764.3370 - val_mse: 13883.6426\n",
      "Epoch 48/50\n",
      "2100/2100 [==============================] - 0s 74us/step - loss: -1765.1120 - mse: 14044.1289 - val_loss: -1764.3370 - val_mse: 13883.6426\n",
      "Epoch 49/50\n",
      "2100/2100 [==============================] - 0s 74us/step - loss: -1765.1120 - mse: 14044.1318 - val_loss: -1764.3370 - val_mse: 13883.6426\n",
      "Epoch 50/50\n",
      "2100/2100 [==============================] - 0s 73us/step - loss: -1765.1120 - mse: 14044.1279 - val_loss: -1764.3370 - val_mse: 13883.6426\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x1a3d20fa90>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = models.Sequential()\n",
    "model.add(layers.Dense(150, input_dim=6))\n",
    "model.add(layers.LeakyReLU(alpha=0.05))\n",
    "model.add(layers.Dense(50))\n",
    "model.add(layers.LeakyReLU(alpha=0.05))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(optimizer='rmsprop',loss='binary_crossentropy',metrics=['mse'])\n",
    "\n",
    "model.fit(X_train, y_train, epochs=50, batch_size=50, validation_data = (X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2100 samples, validate on 900 samples\n",
      "Epoch 1/70\n",
      "2100/2100 [==============================] - 1s 553us/step - loss: 14055.3456 - mse: 14055.3477 - val_loss: 13883.7135 - val_mse: 13883.7119\n",
      "Epoch 2/70\n",
      "2100/2100 [==============================] - 0s 107us/step - loss: 14044.1471 - mse: 14044.1455 - val_loss: 13883.6452 - val_mse: 13883.6445\n",
      "Epoch 3/70\n",
      "2100/2100 [==============================] - 0s 110us/step - loss: 14044.1301 - mse: 14044.1328 - val_loss: 13883.6444 - val_mse: 13883.6436\n",
      "Epoch 4/70\n",
      "2100/2100 [==============================] - 0s 104us/step - loss: 14044.1297 - mse: 14044.1299 - val_loss: 13883.6444 - val_mse: 13883.6436\n",
      "Epoch 5/70\n",
      "2100/2100 [==============================] - 0s 104us/step - loss: 14044.1297 - mse: 14044.1328 - val_loss: 13883.6444 - val_mse: 13883.6436\n",
      "Epoch 6/70\n",
      "2100/2100 [==============================] - 0s 104us/step - loss: 14044.1296 - mse: 14044.1309 - val_loss: 13883.6444 - val_mse: 13883.6436\n",
      "Epoch 7/70\n",
      "2100/2100 [==============================] - 0s 102us/step - loss: 14044.1297 - mse: 14044.1279 - val_loss: 13883.6444 - val_mse: 13883.6436\n",
      "Epoch 8/70\n",
      "2100/2100 [==============================] - 0s 104us/step - loss: 14044.1297 - mse: 14044.1318 - val_loss: 13883.6444 - val_mse: 13883.6436\n",
      "Epoch 9/70\n",
      "2100/2100 [==============================] - 0s 105us/step - loss: 14044.1298 - mse: 14044.1309 - val_loss: 13883.6444 - val_mse: 13883.6436\n",
      "Epoch 10/70\n",
      "2100/2100 [==============================] - 0s 124us/step - loss: 14044.1298 - mse: 14044.1299 - val_loss: 13883.6444 - val_mse: 13883.6436\n",
      "Epoch 11/70\n",
      "2100/2100 [==============================] - 0s 133us/step - loss: 14044.1298 - mse: 14044.1289 - val_loss: 13883.6444 - val_mse: 13883.6436\n",
      "Epoch 12/70\n",
      "2100/2100 [==============================] - 0s 145us/step - loss: 14044.1297 - mse: 14044.1309 - val_loss: 13883.6444 - val_mse: 13883.6436\n",
      "Epoch 13/70\n",
      "2100/2100 [==============================] - 0s 121us/step - loss: 14044.1298 - mse: 14044.1289 - val_loss: 13883.6444 - val_mse: 13883.6436\n",
      "Epoch 14/70\n",
      "2100/2100 [==============================] - 0s 121us/step - loss: 14044.1298 - mse: 14044.1299 - val_loss: 13883.6444 - val_mse: 13883.6436\n",
      "Epoch 15/70\n",
      "2100/2100 [==============================] - 0s 145us/step - loss: 14044.1298 - mse: 14044.1309 - val_loss: 13883.6444 - val_mse: 13883.6436\n",
      "Epoch 16/70\n",
      "2100/2100 [==============================] - 0s 128us/step - loss: 14044.1297 - mse: 14044.1318 - val_loss: 13883.6444 - val_mse: 13883.6436\n",
      "Epoch 17/70\n",
      "2100/2100 [==============================] - 0s 130us/step - loss: 14044.1297 - mse: 14044.1299 - val_loss: 13883.6444 - val_mse: 13883.6436\n",
      "Epoch 18/70\n",
      "2100/2100 [==============================] - 0s 130us/step - loss: 14044.1297 - mse: 14044.1309 - val_loss: 13883.6444 - val_mse: 13883.6436\n",
      "Epoch 19/70\n",
      "2100/2100 [==============================] - 0s 111us/step - loss: 14044.1297 - mse: 14044.1299 - val_loss: 13883.6444 - val_mse: 13883.6436\n",
      "Epoch 20/70\n",
      "2100/2100 [==============================] - 0s 132us/step - loss: 14044.1296 - mse: 14044.1338 - val_loss: 13883.6444 - val_mse: 13883.6436\n",
      "Epoch 21/70\n",
      "2100/2100 [==============================] - 0s 128us/step - loss: 14044.1297 - mse: 14044.1309 - val_loss: 13883.6444 - val_mse: 13883.6436\n",
      "Epoch 22/70\n",
      "2100/2100 [==============================] - 0s 118us/step - loss: 14044.1296 - mse: 14044.1289 - val_loss: 13883.6444 - val_mse: 13883.6436\n",
      "Epoch 23/70\n",
      "2100/2100 [==============================] - 0s 144us/step - loss: 14044.1298 - mse: 14044.1299 - val_loss: 13883.6444 - val_mse: 13883.6436\n",
      "Epoch 24/70\n",
      "2100/2100 [==============================] - 0s 111us/step - loss: 14044.1297 - mse: 14044.1289 - val_loss: 13883.6444 - val_mse: 13883.6436\n",
      "Epoch 25/70\n",
      "2100/2100 [==============================] - 0s 142us/step - loss: 14044.1296 - mse: 14044.1309 - val_loss: 13883.6444 - val_mse: 13883.6436\n",
      "Epoch 26/70\n",
      "2100/2100 [==============================] - 0s 131us/step - loss: 14044.1297 - mse: 14044.1318 - val_loss: 13883.6444 - val_mse: 13883.6436\n",
      "Epoch 27/70\n",
      "2100/2100 [==============================] - 0s 151us/step - loss: 14044.1298 - mse: 14044.1289 - val_loss: 13883.6444 - val_mse: 13883.6436\n",
      "Epoch 28/70\n",
      "2100/2100 [==============================] - 0s 130us/step - loss: 14044.1297 - mse: 14044.1299 - val_loss: 13883.6444 - val_mse: 13883.6436\n",
      "Epoch 29/70\n",
      "2100/2100 [==============================] - 0s 141us/step - loss: 14044.1297 - mse: 14044.1299 - val_loss: 13883.6444 - val_mse: 13883.6436\n",
      "Epoch 30/70\n",
      "2100/2100 [==============================] - 0s 150us/step - loss: 14044.1297 - mse: 14044.1279 - val_loss: 13883.6444 - val_mse: 13883.6436\n",
      "Epoch 31/70\n",
      "2100/2100 [==============================] - 0s 143us/step - loss: 14044.1297 - mse: 14044.1309 - val_loss: 13883.6444 - val_mse: 13883.6436\n",
      "Epoch 32/70\n",
      "2100/2100 [==============================] - 0s 125us/step - loss: 14044.1297 - mse: 14044.1289 - val_loss: 13883.6444 - val_mse: 13883.6436\n",
      "Epoch 33/70\n",
      "2100/2100 [==============================] - 0s 110us/step - loss: 14044.1297 - mse: 14044.1279 - val_loss: 13883.6444 - val_mse: 13883.6436\n",
      "Epoch 34/70\n",
      "2100/2100 [==============================] - 0s 125us/step - loss: 14044.1298 - mse: 14044.1299 - val_loss: 13883.6444 - val_mse: 13883.6436\n",
      "Epoch 35/70\n",
      "2100/2100 [==============================] - 0s 116us/step - loss: 14044.1297 - mse: 14044.1289 - val_loss: 13883.6444 - val_mse: 13883.6436\n",
      "Epoch 36/70\n",
      "2100/2100 [==============================] - 0s 110us/step - loss: 14044.1298 - mse: 14044.1318 - val_loss: 13883.6444 - val_mse: 13883.6436\n",
      "Epoch 37/70\n",
      "2100/2100 [==============================] - 0s 112us/step - loss: 14044.1298 - mse: 14044.1318 - val_loss: 13883.6444 - val_mse: 13883.6436\n",
      "Epoch 38/70\n",
      "2100/2100 [==============================] - 0s 107us/step - loss: 14044.1297 - mse: 14044.1289 - val_loss: 13883.6444 - val_mse: 13883.6436\n",
      "Epoch 39/70\n",
      "2100/2100 [==============================] - 0s 122us/step - loss: 14044.1298 - mse: 14044.1318 - val_loss: 13883.6444 - val_mse: 13883.6436\n",
      "Epoch 40/70\n",
      "2100/2100 [==============================] - 0s 111us/step - loss: 14044.1297 - mse: 14044.1299 - val_loss: 13883.6444 - val_mse: 13883.6436\n",
      "Epoch 41/70\n",
      "2100/2100 [==============================] - 0s 108us/step - loss: 14044.1297 - mse: 14044.1279 - val_loss: 13883.6444 - val_mse: 13883.6436\n",
      "Epoch 42/70\n",
      "2100/2100 [==============================] - 0s 110us/step - loss: 14044.1298 - mse: 14044.1299 - val_loss: 13883.6444 - val_mse: 13883.6436\n",
      "Epoch 43/70\n",
      "2100/2100 [==============================] - 0s 113us/step - loss: 14044.1297 - mse: 14044.1299 - val_loss: 13883.6444 - val_mse: 13883.6436\n",
      "Epoch 44/70\n",
      "2100/2100 [==============================] - 0s 107us/step - loss: 14044.1297 - mse: 14044.1328 - val_loss: 13883.6444 - val_mse: 13883.6436\n",
      "Epoch 45/70\n",
      "2100/2100 [==============================] - 0s 106us/step - loss: 14044.1297 - mse: 14044.1309 - val_loss: 13883.6444 - val_mse: 13883.6436\n",
      "Epoch 46/70\n",
      "2100/2100 [==============================] - 0s 109us/step - loss: 14044.1298 - mse: 14044.1299 - val_loss: 13883.6444 - val_mse: 13883.6436\n",
      "Epoch 47/70\n",
      "2100/2100 [==============================] - 0s 147us/step - loss: 14044.1297 - mse: 14044.1289 - val_loss: 13883.6444 - val_mse: 13883.6436\n",
      "Epoch 48/70\n",
      "2100/2100 [==============================] - 0s 137us/step - loss: 14044.1298 - mse: 14044.1279 - val_loss: 13883.6444 - val_mse: 13883.6436\n",
      "Epoch 49/70\n",
      "2100/2100 [==============================] - 0s 109us/step - loss: 14044.1297 - mse: 14044.1279 - val_loss: 13883.6444 - val_mse: 13883.6436\n",
      "Epoch 50/70\n",
      "2100/2100 [==============================] - 0s 122us/step - loss: 14044.1296 - mse: 14044.1299 - val_loss: 13883.6444 - val_mse: 13883.6436\n",
      "Epoch 51/70\n",
      "2100/2100 [==============================] - 0s 119us/step - loss: 14044.1298 - mse: 14044.1289 - val_loss: 13883.6444 - val_mse: 13883.6436\n",
      "Epoch 52/70\n",
      "2100/2100 [==============================] - 0s 105us/step - loss: 14044.1298 - mse: 14044.1289 - val_loss: 13883.6444 - val_mse: 13883.6436\n",
      "Epoch 53/70\n",
      "2100/2100 [==============================] - 0s 113us/step - loss: 14044.1298 - mse: 14044.1299 - val_loss: 13883.6444 - val_mse: 13883.6436\n",
      "Epoch 54/70\n",
      "2100/2100 [==============================] - 0s 105us/step - loss: 14044.1298 - mse: 14044.1289 - val_loss: 13883.6444 - val_mse: 13883.6436\n",
      "Epoch 55/70\n",
      "2100/2100 [==============================] - 0s 100us/step - loss: 14044.1297 - mse: 14044.1289 - val_loss: 13883.6444 - val_mse: 13883.6436\n",
      "Epoch 56/70\n",
      "2100/2100 [==============================] - 0s 113us/step - loss: 14044.1298 - mse: 14044.1299 - val_loss: 13883.6444 - val_mse: 13883.6436\n",
      "Epoch 57/70\n",
      "2100/2100 [==============================] - 0s 102us/step - loss: 14044.1297 - mse: 14044.1289 - val_loss: 13883.6444 - val_mse: 13883.6436\n",
      "Epoch 58/70\n",
      "2100/2100 [==============================] - 0s 106us/step - loss: 14044.1298 - mse: 14044.1299 - val_loss: 13883.6444 - val_mse: 13883.6436\n",
      "Epoch 59/70\n",
      "2100/2100 [==============================] - 0s 101us/step - loss: 14044.1297 - mse: 14044.1328 - val_loss: 13883.6444 - val_mse: 13883.6436\n",
      "Epoch 60/70\n",
      "2100/2100 [==============================] - 0s 101us/step - loss: 14044.1297 - mse: 14044.1279 - val_loss: 13883.6444 - val_mse: 13883.6436\n",
      "Epoch 61/70\n",
      "2100/2100 [==============================] - 0s 125us/step - loss: 14044.1298 - mse: 14044.1309 - val_loss: 13883.6444 - val_mse: 13883.6436\n",
      "Epoch 62/70\n",
      "2100/2100 [==============================] - 0s 111us/step - loss: 14044.1298 - mse: 14044.1299 - val_loss: 13883.6444 - val_mse: 13883.6436\n",
      "Epoch 63/70\n",
      "2100/2100 [==============================] - 0s 100us/step - loss: 14044.1298 - mse: 14044.1318 - val_loss: 13883.6444 - val_mse: 13883.6436\n",
      "Epoch 64/70\n",
      "2100/2100 [==============================] - 0s 109us/step - loss: 14044.1298 - mse: 14044.1299 - val_loss: 13883.6444 - val_mse: 13883.6436\n",
      "Epoch 65/70\n",
      "2100/2100 [==============================] - 0s 110us/step - loss: 14044.1298 - mse: 14044.1318 - val_loss: 13883.6444 - val_mse: 13883.6436\n",
      "Epoch 66/70\n",
      "2100/2100 [==============================] - 0s 103us/step - loss: 14044.1297 - mse: 14044.1270 - val_loss: 13883.6444 - val_mse: 13883.6436\n",
      "Epoch 67/70\n",
      "2100/2100 [==============================] - 0s 101us/step - loss: 14044.1297 - mse: 14044.1279 - val_loss: 13883.6444 - val_mse: 13883.6436\n",
      "Epoch 68/70\n",
      "2100/2100 [==============================] - 0s 104us/step - loss: 14044.1297 - mse: 14044.1318 - val_loss: 13883.6444 - val_mse: 13883.6436\n",
      "Epoch 69/70\n",
      "2100/2100 [==============================] - 0s 109us/step - loss: 14044.1297 - mse: 14044.1289 - val_loss: 13883.6444 - val_mse: 13883.6436\n",
      "Epoch 70/70\n",
      "2100/2100 [==============================] - 0s 100us/step - loss: 14044.1297 - mse: 14044.1318 - val_loss: 13883.6444 - val_mse: 13883.6436\n",
      "MSE of Neural Network: 13883.644275025523\n"
     ]
    }
   ],
   "source": [
    "network = models.Sequential()\n",
    "network.add(layers.Dense(150, input_dim=6, activation='relu'))\n",
    "network.add(layers.Dense(50, activation='relu'))\n",
    "network.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "network.compile(optimizer='rmsprop',\n",
    "loss='mse',\n",
    "metrics=['mse'])\n",
    "\n",
    "history=network.fit(X_train, y_train, epochs=70, batch_size=32, validation_data = (X_test, y_test))\n",
    "y_pred = network.predict(X_test)\n",
    "print('MSE of Neural Network:',mean_squared_error(y_test,y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2100 samples, validate on 900 samples\n",
      "Epoch 1/20\n",
      "2100/2100 [==============================] - 1s 675us/step - loss: 14045.1408 - mse: 14045.1387 - val_loss: 13883.7359 - val_mse: 13883.7344\n",
      "Epoch 2/20\n",
      "2100/2100 [==============================] - 1s 255us/step - loss: 14044.1938 - mse: 14044.1934 - val_loss: 13883.6873 - val_mse: 13883.6875\n",
      "Epoch 3/20\n",
      "2100/2100 [==============================] - 1s 259us/step - loss: 14044.1657 - mse: 14044.1699 - val_loss: 13883.6718 - val_mse: 13883.6709\n",
      "Epoch 4/20\n",
      "2100/2100 [==============================] - 1s 261us/step - loss: 14044.1546 - mse: 14044.1572 - val_loss: 13883.6643 - val_mse: 13883.6641\n",
      "Epoch 5/20\n",
      "2100/2100 [==============================] - 1s 258us/step - loss: 14044.1487 - mse: 14044.1504 - val_loss: 13883.6598 - val_mse: 13883.6611\n",
      "Epoch 6/20\n",
      "2100/2100 [==============================] - 1s 259us/step - loss: 14044.1451 - mse: 14044.1445 - val_loss: 13883.6570 - val_mse: 13883.6582\n",
      "Epoch 7/20\n",
      "2100/2100 [==============================] - 1s 260us/step - loss: 14044.1424 - mse: 14044.1426 - val_loss: 13883.6549 - val_mse: 13883.6553\n",
      "Epoch 8/20\n",
      "2100/2100 [==============================] - 1s 305us/step - loss: 14044.1406 - mse: 14044.1445 - val_loss: 13883.6535 - val_mse: 13883.6553\n",
      "Epoch 9/20\n",
      "2100/2100 [==============================] - 1s 301us/step - loss: 14044.1394 - mse: 14044.1436 - val_loss: 13883.6525 - val_mse: 13883.6533\n",
      "Epoch 10/20\n",
      "2100/2100 [==============================] - 1s 329us/step - loss: 14044.1382 - mse: 14044.1338 - val_loss: 13883.6517 - val_mse: 13883.6533\n",
      "Epoch 11/20\n",
      "2100/2100 [==============================] - 1s 303us/step - loss: 14044.1372 - mse: 14044.1367 - val_loss: 13883.6509 - val_mse: 13883.6523\n",
      "Epoch 12/20\n",
      "2100/2100 [==============================] - 1s 301us/step - loss: 14044.1366 - mse: 14044.1426 - val_loss: 13883.6502 - val_mse: 13883.6523\n",
      "Epoch 13/20\n",
      "2100/2100 [==============================] - 1s 286us/step - loss: 14044.1360 - mse: 14044.1377 - val_loss: 13883.6497 - val_mse: 13883.6523\n",
      "Epoch 14/20\n",
      "2100/2100 [==============================] - 1s 274us/step - loss: 14044.1355 - mse: 14044.1357 - val_loss: 13883.6493 - val_mse: 13883.6523\n",
      "Epoch 15/20\n",
      "2100/2100 [==============================] - 1s 276us/step - loss: 14044.1351 - mse: 14044.1338 - val_loss: 13883.6489 - val_mse: 13883.6523\n",
      "Epoch 16/20\n",
      "2100/2100 [==============================] - 1s 272us/step - loss: 14044.1348 - mse: 14044.1348 - val_loss: 13883.6487 - val_mse: 13883.6523\n",
      "Epoch 17/20\n",
      "2100/2100 [==============================] - 1s 279us/step - loss: 14044.1345 - mse: 14044.1328 - val_loss: 13883.6484 - val_mse: 13883.6523\n",
      "Epoch 18/20\n",
      "2100/2100 [==============================] - 1s 274us/step - loss: 14044.1342 - mse: 14044.1289 - val_loss: 13883.6481 - val_mse: 13883.6514\n",
      "Epoch 19/20\n",
      "2100/2100 [==============================] - 1s 382us/step - loss: 14044.1340 - mse: 14044.1328 - val_loss: 13883.6478 - val_mse: 13883.6514\n",
      "Epoch 20/20\n",
      "2100/2100 [==============================] - 1s 336us/step - loss: 14044.1336 - mse: 14044.1309 - val_loss: 13883.6476 - val_mse: 13883.6514\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x1a3fd0a7d0>"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras import optimizers\n",
    "network = models.Sequential()\n",
    "network.add(layers.Dense(100, input_dim=6, activation='relu'))\n",
    "network.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "opt = optimizers.SGD(learning_rate=0.001)\n",
    "network.compile(optimizer=opt,\n",
    "loss='mse',\n",
    "metrics=['mse'])\n",
    "\n",
    "network.fit(X_train, y_train, epochs=20, batch_size=10, validation_data = (X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2100 samples, validate on 900 samples\n",
      "Epoch 1/20\n",
      "2100/2100 [==============================] - 1s 701us/step - loss: 2992.2274 - mse: 2992.2268 - val_loss: 1274.6928 - val_mse: 1274.6927\n",
      "Epoch 2/20\n",
      "2100/2100 [==============================] - 1s 261us/step - loss: 4340.4702 - mse: 4340.4712 - val_loss: 3123.1274 - val_mse: 3123.1277\n",
      "Epoch 3/20\n",
      "2100/2100 [==============================] - 1s 266us/step - loss: 3517.2925 - mse: 3517.2930 - val_loss: 1641.0011 - val_mse: 1641.0009\n",
      "Epoch 4/20\n",
      "2100/2100 [==============================] - 1s 262us/step - loss: 2077.5957 - mse: 2077.5959 - val_loss: 1211.6528 - val_mse: 1211.6526\n",
      "Epoch 5/20\n",
      "2100/2100 [==============================] - 1s 261us/step - loss: 3045.2096 - mse: 3045.2104 - val_loss: 2652.1016 - val_mse: 2652.1023\n",
      "Epoch 6/20\n",
      "2100/2100 [==============================] - 1s 261us/step - loss: 1866.7307 - mse: 1866.7308 - val_loss: 4146.2728 - val_mse: 4146.2734\n",
      "Epoch 7/20\n",
      "2100/2100 [==============================] - 1s 258us/step - loss: 2129.5783 - mse: 2129.5781 - val_loss: 1296.7954 - val_mse: 1296.7954\n",
      "Epoch 8/20\n",
      "2100/2100 [==============================] - 1s 263us/step - loss: 2348.8407 - mse: 2348.8408 - val_loss: 1443.5994 - val_mse: 1443.5997\n",
      "Epoch 9/20\n",
      "2100/2100 [==============================] - 1s 262us/step - loss: 1723.5540 - mse: 1723.5538 - val_loss: 1664.8445 - val_mse: 1664.8446\n",
      "Epoch 10/20\n",
      "2100/2100 [==============================] - 1s 266us/step - loss: 1685.4928 - mse: 1685.4932 - val_loss: 1432.8161 - val_mse: 1432.8163\n",
      "Epoch 11/20\n",
      "2100/2100 [==============================] - 1s 377us/step - loss: 1813.9264 - mse: 1813.9261 - val_loss: 1401.4162 - val_mse: 1401.4159\n",
      "Epoch 12/20\n",
      "2100/2100 [==============================] - 1s 324us/step - loss: 1721.5696 - mse: 1721.5698 - val_loss: 1560.5857 - val_mse: 1560.5858\n",
      "Epoch 13/20\n",
      "2100/2100 [==============================] - 1s 383us/step - loss: 3725.6421 - mse: 3725.6421 - val_loss: 4699.6761 - val_mse: 4699.6763\n",
      "Epoch 14/20\n",
      "2100/2100 [==============================] - 1s 413us/step - loss: 3865.6095 - mse: 3865.6106 - val_loss: 2957.4003 - val_mse: 2957.4006\n",
      "Epoch 15/20\n",
      "2100/2100 [==============================] - 1s 337us/step - loss: 1959.0722 - mse: 1959.0720 - val_loss: 1207.1771 - val_mse: 1207.1770\n",
      "Epoch 16/20\n",
      "2100/2100 [==============================] - 1s 323us/step - loss: 1358.4693 - mse: 1358.4692 - val_loss: 1465.2303 - val_mse: 1465.2302\n",
      "Epoch 17/20\n",
      "2100/2100 [==============================] - 1s 299us/step - loss: 1392.3995 - mse: 1392.3992 - val_loss: 1288.6588 - val_mse: 1288.6586\n",
      "Epoch 18/20\n",
      "2100/2100 [==============================] - 1s 270us/step - loss: 1406.6528 - mse: 1406.6532 - val_loss: 1218.5403 - val_mse: 1218.5403\n",
      "Epoch 19/20\n",
      "2100/2100 [==============================] - 1s 271us/step - loss: 1434.1361 - mse: 1434.1367 - val_loss: 1179.8336 - val_mse: 1179.8334\n",
      "Epoch 20/20\n",
      "2100/2100 [==============================] - 1s 267us/step - loss: 1384.9268 - mse: 1384.9264 - val_loss: 1160.2211 - val_mse: 1160.2214\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x1a3eb34ad0>"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "network = models.Sequential()\n",
    "network.add(layers.Dense(100, input_dim=6, activation='relu'))\n",
    "network.add(layers.Dense(1, activation='linear'))\n",
    "\n",
    "opt = optimizers.SGD(learning_rate=0.001)\n",
    "network.compile(optimizer=opt,\n",
    "loss='mse',\n",
    "metrics=['mse'])\n",
    "\n",
    "network.fit(X_train, y_train, epochs=20, batch_size=10, validation_data = (X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     variable   estimate\n",
      "0      maritl   2.733495\n",
      "1        race  -1.053450\n",
      "2   education  17.186703\n",
      "3    jobclass   2.069845\n",
      "4      health   2.077406\n",
      "5  health_ins  -8.624814\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "\n",
    "feature_names =['maritl', 'race', 'education','jobclass','health','health_ins']\n",
    "\n",
    "\n",
    "data = preprocessing.scale(df[feature_names])\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, df.wage, test_size=0.3, random_state=10)\n",
    "\n",
    "ols_reg = LinearRegression()\n",
    "ols_reg.fit(X_train, y_train)\n",
    "ols_pred = ols_reg.predict(X_test)\n",
    "\n",
    "print(pd.DataFrame({'variable': feature_names, 'estimate': ols_reg.coef_}))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "ridge_reg = Ridge(alpha=1)\n",
    "ridge_reg.fit(X_train, y_train)\n",
    "ridge_df = pd.DataFrame({'variable': ['maritl', 'race', 'education','jobclass','health','health_ins'], 'estimate': ridge_reg.coef_})\n",
    "ridge_train_pred = []\n",
    "ridge_test_pred = []\n",
    "\n",
    "alphas = np.arange(0, 200, 1)\n",
    "\n",
    "for alpha in alphas:\n",
    "    ridge_reg = Ridge(alpha=alpha)\n",
    "    ridge_reg.fit(X_train, y_train)\n",
    "    var_name = 'estimate' + str(alpha)\n",
    "    ridge_df[var_name] = ridge_reg.coef_\n",
    "    # prediction\n",
    "    ridge_train_pred.append(ridge_reg.predict(X_train))\n",
    "    ridge_test_pred.append(ridge_reg.predict(X_test))\n",
    "    \n",
    "ridge_df = ridge_df.set_index('variable').T.rename_axis('estimate').reset_index()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "ax.plot(ridge_df.maritl, 'r', ridge_df.race, 'g', ridge_df.education, 'b', ridge_df.jobclass, 'c', ridge_df.health, 'y', ridge_df.health_ins, 'm')\n",
    "ax.axhline(y=0, color='black', linestyle='--')\n",
    "ax.set_xlabel(\"Lambda\")\n",
    "ax.set_ylabel(\"Beta Estimate\")\n",
    "ax.set_title(\"Ridge Regression Trace\", fontsize=16)\n",
    "ax.legend(labels=['Maritl','Race','Education','Job Class','Health','Health Insurance'])\n",
    "ax.grid(True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'MSE')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# MSE of training set\n",
    "ridge_mse_train = [mean_squared_error(y_train, p) for p in ridge_train_pred]\n",
    "\n",
    "# MSE of test set\n",
    "ridge_mse_test = [mean_squared_error(y_test, p) for p in ridge_test_pred]\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "plt.rcParams['axes.grid'] = True\n",
    "ax.plot(ridge_mse_train, 'b', ridge_mse_test, 'r')\n",
    "ax.set_title(\"Ridge Regression MSE\", fontsize=16)\n",
    "ax.set_ylabel(\"MSE\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
